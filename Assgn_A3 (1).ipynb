{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ba44ff3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import MaxPooling2D"
      ],
      "id": "ba44ff3f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "747543ec",
        "outputId": "5a5c1534-b669-4fbc-8404-d4ef2533770d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "id": "747543ec"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1978e9d",
        "outputId": "62c1aa24-b289-4d84-ddff-f90c4f5848a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "X_train.shape"
      ],
      "id": "e1978e9d"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "614cf41e"
      },
      "outputs": [],
      "source": [
        "X_train = X_train[:, np.newaxis, :, :]\n",
        "X_test = X_test[:, np.newaxis, :, :]"
      ],
      "id": "614cf41e"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0912b18",
        "outputId": "4bd2a650-50b8-468a-973b-8093ce5200a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X_train.shape"
      ],
      "id": "b0912b18"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dfbedb34"
      },
      "outputs": [],
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "id": "dfbedb34"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7a0d5fb7"
      },
      "outputs": [],
      "source": [
        "mx_acc=[]"
      ],
      "id": "7a0d5fb7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a66724de"
      },
      "outputs": [],
      "source": [
        "a=[SGD,RMSprop,Adam]"
      ],
      "id": "a66724de"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e58ef7c",
        "outputId": "559d1bf8-a84e-4679-aa99-54d595135f69",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2247 - accuracy: 0.9316\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.7804 - accuracy: 0.7519\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5230 - accuracy: 0.5415\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448}\n",
            "313/313 [==============================] - 3s 9ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 9ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0973 - accuracy: 0.9723\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0689 - accuracy: 0.9780\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0854 - accuracy: 0.9743\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 285954879663424667648.0000 - accuracy: 0.2853\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 222381557150067458048.0000 - accuracy: 0.1578\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 39665013913414533120.0000 - accuracy: 0.6199\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 30453833554222513379082240.0000 - accuracy: 0.0956\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 192771934334778634928128.0000 - accuracy: 0.2223\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 444390783704541687709696.0000 - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0833 - accuracy: 0.9742\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0758 - accuracy: 0.9790\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0776 - accuracy: 0.9747\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586}\n",
            "313/313 [==============================] - 3s 10ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 700): 0.09799999743700027}\n"
          ]
        }
      ],
      "source": [
        "acc={}\n",
        "for f in a:\n",
        "    for i in np.arange(0.001,1,0.4):\n",
        "        for j in np.arange(100,800,300):\n",
        "            model1 = Sequential()\n",
        "            model1.add(Conv2D(10, kernel_size=3, padding=\"same\", input_shape=(1,28,28)))\n",
        "            model1.add(Conv2D(20, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model1.add(Conv2D(30, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(Conv2D(40, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model1.add(Conv2D(50, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(Conv2D(60, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model1.add(Conv2D(70, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(Conv2D(80, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model1.add(Conv2D(90, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(Conv2D(100, kernel_size=3, padding=\"same\"))\n",
        "            model1.add(Flatten())\n",
        "            model1.add(Dense(10))\n",
        "            model1.add(Activation('softmax'))\n",
        "            model1.compile(loss='categorical_crossentropy', optimizer=f(lr=i), metrics=['accuracy'])\n",
        "            model1.fit(X_train, Y_train, batch_size=j, epochs=4,verbose=0)\n",
        "            score = model1.evaluate(X_test, Y_test)\n",
        "            acc[(f,i,j)]=score[1]\n",
        "            print(acc)"
      ],
      "id": "6e58ef7c"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2031d08d"
      },
      "outputs": [],
      "source": [
        "l=[]\n",
        "for i in \"{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315999746322632, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.7519000172615051, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5414999723434448, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9722999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9779999852180481, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.28529998660087585, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.15780000388622284, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.6198999881744385, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09560000151395798, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.2222999930381775, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9742000102996826, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9789999723434448, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9746999740600586, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 700): 0.09799999743700027}\".split(\", (\"):\n",
        "    l.append(i)\n",
        "l[0]=l[0][1:]\n",
        "l[len(l)-1]=l[len(l)-1][:len(l[len(l)-1])]"
      ],
      "id": "2031d08d"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "867785d5"
      },
      "outputs": [],
      "source": [
        "mod=[]\n",
        "ac=[]\n",
        "for i in l:\n",
        "    a1,b=i.split(\":\")\n",
        "    mod.append(a1[len(a1)-19:len(a1)-1])\n",
        "    ac.append(float(b[:len(b)-2].strip()))\n",
        "mod[0]=mod[0][1:]"
      ],
      "id": "867785d5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "299618f5",
        "outputId": "19bb3e9c-677c-4946-acf8-57a40513909d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 27 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4320x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f=plt.figure()\n",
        "f.set_figheight(6)\n",
        "f.set_figwidth(60)\n",
        "plt.bar(mod,ac)"
      ],
      "id": "299618f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4633ba1d"
      },
      "source": [
        "### FROM the plot for model-1 we can see that \n",
        "### FOR SGD           \n",
        "the optimal batch size is 100 which is yileding good accuracy\n",
        "the optimal learning rate is 0.001 since for other values the accuracy is quite depressing\n",
        "### FOR rmsprop\n",
        "there is no much impact of batch size. Almost all batch sizes are producing simmilar kind of accuracy\n",
        "the optimal learning rate is 0.001 since for other values the accuracy is quite depressing\n",
        "### FOR Adam\n",
        "there is no much impact of batch size. Almost all batch sizes are producing simmilar kind of accuracy\n",
        "the optimal learning rate is 0.001 since for other values the accuracy is quite depressing\n",
        "\n",
        "### OVERALL\n",
        "Finally we can see the with lower value of learning rate, all model-1 are performoing good\n",
        "There is no much impact of batch size for Adam and rmsprop but the SGD is being affected by batch size"
      ],
      "id": "4633ba1d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c1bbc4f"
      },
      "source": [
        "### max accuracy producing parameters for first model"
      ],
      "id": "5c1bbc4f"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18f370a4",
        "outputId": "c713f94e-cb40-4e66-fb98-ec13f26c183c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam'>, 0.001, 400\n",
            "0.98000001907348\n"
          ]
        }
      ],
      "source": [
        "print(mod[ac.index(max(ac))])\n",
        "print(max(ac))"
      ],
      "id": "18f370a4"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7a1d28b8"
      },
      "outputs": [],
      "source": [
        "maxval=0\n",
        "for i,j in acc.items():\n",
        "    if(j>maxval):\n",
        "        maxval=j\n",
        "        z=i\n",
        "mx_acc.append(z)    "
      ],
      "id": "7a1d28b8"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d344d83d",
        "outputId": "1872269e-664c-4143-92f7-85c2a9a30215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2230 - accuracy: 0.9315\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.3021 - accuracy: 0.6143\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.6822 - accuracy: 0.5541\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0715 - accuracy: 0.9768\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0761 - accuracy: 0.9759\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.1126 - accuracy: 0.9647\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 585683868154068992.0000 - accuracy: 0.5899\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 349189980855205888.0000 - accuracy: 0.2973\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 16678720588742656.0000 - accuracy: 0.3849\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 82389986279115718656.0000 - accuracy: 0.9057\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575}\n",
            "313/313 [==============================] - 3s 7ms/step - loss: 2884375417345707868160.0000 - accuracy: 0.4791\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1447725983293015037313024.0000 - accuracy: 0.1585\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503}\n",
            "313/313 [==============================] - 3s 7ms/step - loss: 0.0958 - accuracy: 0.9725\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0670 - accuracy: 0.9783\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026}\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0842 - accuracy: 0.9707\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717}\n",
            "313/313 [==============================] - 3s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 7ms/step - loss: 82676610498560.0000 - accuracy: 0.4445\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.44449999928474426}\n",
            "313/313 [==============================] - 3s 7ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.44449999928474426, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.44449999928474426, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.44449999928474426, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 700): 0.09799999743700027}\n"
          ]
        }
      ],
      "source": [
        "acc={}\n",
        "for f in a:\n",
        "    for i in np.arange(0.001,1,0.4):\n",
        "        for j in np.arange(100,800,300):\n",
        "            model3 = Sequential()\n",
        "            model3.add(Conv2D(10, kernel_size=3, padding=\"same\", input_shape=(1,28,28)))\n",
        "            model3.add(Conv2D(20, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model3.add(Conv2D(30, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(Conv2D(40, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model3.add(Conv2D(50, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(Conv2D(60, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model3.add(Conv2D(50, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(Conv2D(40, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model3.add(Conv2D(30, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(Conv2D(20, kernel_size=3, padding=\"same\"))\n",
        "            model3.add(Flatten())\n",
        "            model3.add(Dense(10))\n",
        "            model3.add(Activation('softmax'))\n",
        "            model3.compile(loss='categorical_crossentropy', optimizer=f(lr=i), metrics=['accuracy'])\n",
        "            model3.fit(X_train, Y_train, batch_size=j, epochs=4,verbose=0)\n",
        "            score = model3.evaluate(X_test, Y_test)\n",
        "            acc[(f,i,j)]=score[1]\n",
        "            print(acc)"
      ],
      "id": "d344d83d"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dd8c70ca"
      },
      "outputs": [],
      "source": [
        "l=[]\n",
        "for i in \"{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9315000176429749, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.614300012588501, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.554099977016449, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9768000245094299, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9758999943733215, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9646999835968018, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.589900016784668, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2973000109195709, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.3849000036716461, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.9057000279426575, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.47909998893737793, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.15850000083446503, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9725000262260437, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.9782999753952026, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9707000255584717, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.44449999928474426, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 700): 0.09799999743700027}\".split(\", (\"):\n",
        "    l.append(i)\n",
        "l[0]=l[0][1:]\n",
        "l[len(l)-1]=l[len(l)-1][:len(l[len(l)-1])]"
      ],
      "id": "dd8c70ca"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "798faed4"
      },
      "outputs": [],
      "source": [
        "mod=[]\n",
        "ac=[]\n",
        "for i in l:\n",
        "    a1,b=i.split(\":\")\n",
        "    mod.append(a1[len(a1)-19:len(a1)-1])\n",
        "    ac.append(float(b[:len(b)-2].strip()))\n",
        "mod[0]=mod[0][1:]"
      ],
      "id": "798faed4"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "53f9e307",
        "outputId": "faf3eb52-e47f-4baa-aa14-c33502cb082e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 27 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4320x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f=plt.figure()\n",
        "f.set_figheight(6)\n",
        "f.set_figwidth(60)\n",
        "plt.bar(mod,ac)"
      ],
      "id": "53f9e307"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "334b7df9"
      },
      "source": [
        "### FROM the plot for model-2 we can see that \n",
        "### FOR SGD           \n",
        "the optimal batch size is 100 which is yileding good accuracy\n",
        "the optimal learning rate is 0.001 since for other values the accuracy is quite depressing\n",
        "### FOR rmsprop\n",
        "there is no much impact of batch size. Almost all batch sizes are producing simmilar kind of accuracy\n",
        "the optimal learning rate is 0.001--0.4 since for other values the accuracy is quite depressing\n",
        "### FOR Adam\n",
        "there is no much impact of batch size. Almost all batch sizes are producing simmilar kind of accuracy\n",
        "the optimal learning rate is 0.001-0.4 since for other values the accuracy is quite depressing\n",
        "\n",
        "### OVERALL\n",
        "Finally we can see the with lower value of learning rate, all model2 are performoing good\n",
        "There is no much impact of batch size for Adam and rmsprop but the SGD is being affected by batch size"
      ],
      "id": "334b7df9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5f62d36"
      },
      "source": [
        "### max accuracy producing parameters for second model"
      ],
      "id": "a5f62d36"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68bb64ad",
        "outputId": "ab5470da-4bfe-4cc0-ac95-c657534fd6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmsprop'>, 0.001, 100\n",
            "0.97960001230239\n"
          ]
        }
      ],
      "source": [
        "print(\"rms\"+mod[ac.index(max(ac))])\n",
        "print(max(ac))"
      ],
      "id": "68bb64ad"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0167221a",
        "outputId": "33d45a97-3189-4a02-8524-defe6bd5c22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9782999753952026\n"
          ]
        }
      ],
      "source": [
        "maxval=0\n",
        "for i,j in acc.items():\n",
        "    if(j>maxval):\n",
        "        maxval=j\n",
        "        z=i\n",
        "print(maxval)\n",
        "mx_acc.append(z)    "
      ],
      "id": "0167221a"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6485f88",
        "outputId": "2321a4d3-6310-4a2d-a18f-11ffc98b4713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 14ms/step - loss: 0.1921 - accuracy: 0.9425\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.4844 - accuracy: 0.8640\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.5330 - accuracy: 0.5420\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.0920 - accuracy: 0.9716\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.0544 - accuracy: 0.9823\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0744 - accuracy: 0.9760\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 7924841107869925376.0000 - accuracy: 0.8933\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748}\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 56409516075233837056.0000 - accuracy: 0.2906\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 111052741487236218880.0000 - accuracy: 0.1054\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 15004142366462094075756544.0000 - accuracy: 0.0944\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683}\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 4511774993759663151906816.0000 - accuracy: 0.1225\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 495673790034934717984276480.0000 - accuracy: 0.1010\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.0935 - accuracy: 0.9743\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.0750 - accuracy: 0.9792\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311}\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0649 - accuracy: 0.9800\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863}\n",
            "313/313 [==============================] - 5s 15ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027}\n",
            "313/313 [==============================] - 5s 14ms/step - loss: nan - accuracy: 0.0980\n",
            "{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 700): 0.09799999743700027}\n"
          ]
        }
      ],
      "source": [
        "acc={}\n",
        "for f in a:\n",
        "    for i in np.arange(0.001,1,0.4):\n",
        "        for j in np.arange(100,800,300):    \n",
        "            model2 = Sequential()\n",
        "            model2.add(Conv2D(100, kernel_size=3, padding=\"same\", input_shape=(1,28,28)))\n",
        "            model2.add(Conv2D(90, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model2.add(Conv2D(80, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(Conv2D(70, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model2.add(Conv2D(60, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(Conv2D(50, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model2.add(Conv2D(40, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(Conv2D(30, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
        "            model2.add(Conv2D(20, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(Conv2D(10, kernel_size=3, padding=\"same\"))\n",
        "            model2.add(Flatten())\n",
        "            model2.add(Dense(10))\n",
        "            model2.add(Activation('softmax'))\n",
        "            model2.compile(loss='categorical_crossentropy', optimizer=f(lr=i), metrics=['accuracy'])\n",
        "            model2.fit(X_train, Y_train, batch_size=j, epochs=4,verbose=0)\n",
        "            score = model2.evaluate(X_test, Y_test)\n",
        "            acc[(f,i,j)]=score[1]\n",
        "            print(acc)"
      ],
      "id": "d6485f88"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "23394522"
      },
      "outputs": [],
      "source": [
        "l=[]\n",
        "for i in \"{(<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 100): 0.9424999952316284, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 400): 0.8640000224113464, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.001, 700): 0.5419999957084656, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>, 0.801, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 100): 0.9715999960899353, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400): 0.9822999835014343, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 700): 0.9760000109672546, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 100): 0.8932999968528748, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 400): 0.2906000018119812, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.401, 700): 0.10540000349283218, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 100): 0.09440000355243683, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 400): 0.12250000238418579, (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.801, 700): 0.10100000351667404, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 100): 0.9743000268936157, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400): 0.979200005531311, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700): 0.9800000190734863, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.401, 700): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 100): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 400): 0.09799999743700027, (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.801, 700): 0.09799999743700027}\".split(\", (\"):\n",
        "    l.append(i)\n",
        "l[0]=l[0][1:]\n",
        "l[len(l)-1]=l[len(l)-1][:len(l[len(l)-1])]"
      ],
      "id": "23394522"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5fa6208b"
      },
      "outputs": [],
      "source": [
        "mod=[]\n",
        "ac=[]\n",
        "for i in l:\n",
        "    a1,b=i.split(\":\")\n",
        "    mod.append(a1[len(a1)-19:len(a1)-1])\n",
        "    ac.append(float(b[:len(b)-2].strip()))\n",
        "mod[0]=mod[0][1:]"
      ],
      "id": "5fa6208b"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "8a6b802f",
        "outputId": "7746c5ed-0a8c-4db0-9958-3c4bc720df35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 27 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4320x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAADTkAAAFlCAYAAACatw26AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdX4jdd1rH8c+TjsULV/cisyBN6vQiBYMIuwxlYS8suELaQHohSAOLKMvmxoqwizCyUqXeRAUvhPqngiwKboleSCCRXkhFECudslpsSiXUaFOFjuuyN4vWwuNFZmU2m2SO6Zk8zZnXCw6c3/f35fd7YK4GzptvdXcAAAAAAAAAAAAAAAAAphyZHgAAAAAAAAAAAAAAAAA43EROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwKi1qRcfPXq0NzY2pl4PAAAAAAAAAAAAAAAA3EOvvfbaf3T3+q3ujUVOGxsb2d7enno9AAAAAAAAAAAAAAAAcA9V1b/c7t6RezkIAAAAAAAAAAAAAAAAwM1ETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjNo3cqqqP6yq96rqH29zv6rqt6vqalW9XlWfWv6YAAAAAAAAAAAAAAAAwKpa5CSnryQ5dYf7TyQ5sfs5l+R3P/xYAAAAAAAAAAAAAAAAwGGxb+TU3X+d5D/vsOWpJH/UN7yS5ONV9YPLGhAAAAAAAAAAAAAAAABYbYuc5LSfh5K8s+f6+u4aAAAAAAAAAAAAAAAAwL6WETktrKrOVdV2VW3v7Ozcy1cDAAAAAAAAAAAAAAAAH1HLiJzeTXJ8z/Wx3bXv0t0vdPdmd2+ur68v4dUAAAAAAAAAAAAAAADA/W4ZkdPFJD9dN3w6yTe7+9+X8FwAAAAAAAAAAAAAAADgEFjbb0NVfTXJ40mOVtX1JL+S5HuSpLt/L8nlJE8muZrkW0l+9qCGBQAAAAAAAAAAAAAAAFbPvpFTd5/d534n+bmlTQQAAAAAAAAAAAAAAAAcKvtGTgAAAAAAAAAAABwuG1uXpkc4FK6dPz09AgAAwEfGkekBAAAAAAAAAAAAAAAAgMPNSU4AAAAAAAAAAMDSOAHo3nEKEAAAAKtE5AQAAADAfcWPZO6dg/yRjL/jvePHTgAAwP3E/4v3jv8XAQAAgI8akRMAAAAAAHfFjw/vDT88BAAAAAAAAA6DI9MDAAAAAAAAAAAAAAAAAIebyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYtTY9AByEja1L0yMcGtfOn54eAQAAAAAAAAAAAAAAuM85yQkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYtTY9AMDtbGxdmh7h0Lh2/vT0CAAAAAAAAAAAAAAAHGJOcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGLRQ5VdWpqnqrqq5W1dYt7j9cVS9X1deq6vWqenL5owIAAAAAAAAAAAAAAACraN/IqaoeSPJ8kieSnExytqpO3rTtl5Nc6O5PJnk6ye8se1AAAAAAAAAAAAAAAABgNS1yktNjSa5299vd/X6SF5M8ddOeTvL9u99/IMm/LW9EAAAAAAAAAAAAAAAAYJUtEjk9lOSdPdfXd9f2+tUkn6uq60kuJ/n5Wz2oqs5V1XZVbe/s7NzFuAAAAAAAAAAAAAAAAMCqWSRyWsTZJF/p7mNJnkzyx1X1Xc/u7he6e7O7N9fX15f0agAAAAAAAAAAAAAAAOB+tkjk9G6S43uuj+2u7fX5JBeSpLv/Nsn3Jjm6jAEBAAAAAAAAAAAAAACA1bZI5PRqkhNV9UhVPZjk6SQXb9rzr0l+PEmq6odzI3LaWeagAAAAAAAAAAAAAAAAwGraN3Lq7g+SPJPkpSRvJrnQ3W9U1XNVdWZ325eSfKGq/iHJV5P8THf3QQ0NAAAAAAAAAAAAAAAArI61RTZ19+Ukl29ae3bP9ytJPrPc0QAAAAAAAAAAAAAAAIDDYN+TnAAAAAAAAAAAAAAAAAAOksgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABg1Nr0AAAAAAAAwIyNrUvTIxwa186fnh4BAAAAAAAAPtKc5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBK5AQAAAAAAAAAAAAAAACMEjkBAAAAAAAAAAAAAAAAo0ROAAAAAAAAAAAAAAAAwCiREwAAAAAAAAAAAAAAADBqocipqk5V1VtVdbWqtm6z56eq6kpVvVFVf7LcMQEAAAAAAAAAAAAAAIBVtbbfhqp6IMnzSX4iyfUkr1bVxe6+smfPiSS/lOQz3f2NqvrEQQ0MAAAAAAAAAAAAAAAArJZ9I6ckjyW52t1vJ0lVvZjkqSRX9uz5QpLnu/sbSdLd7y17UADuPxtbl6ZHODSunT89PQIAAAAAAAAAAAAAwF07ssCeh5K8s+f6+u7aXo8mebSq/qaqXqmqU7d6UFWdq6rtqtre2dm5u4kBAAAAAAAAAAAAAACAlbJI5LSItSQnkjye5GySP6iqj9+8qbtf6O7N7t5cX19f0qsBAAAAAAAAAAAAAACA+9kikdO7SY7vuT62u7bX9SQXu/t/uvufk/xTbkRPAAAAAAAAAAAAAAAAAHe0SOT0apITVfVIVT2Y5OkkF2/a8+e5cYpTqupokkeTvL3EOQEAAAAAAAAAAAAAAIAVtW/k1N0fJHkmyUtJ3kxyobvfqKrnqurM7raXkny9qq4keTnJL3b31w9qaAAAAAAAAAAAAAAAAGB1rC2yqbsvJ7l809qze753ki/ufgAAAAAAAAAAAAAAAAAWtu9JTgAAAAAAAAAAAAAAAAAHSeQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjBI5AQAAAAAAAAAAAAAAAKNETgAAAAAAAAAAAAAAAMAokRMAAAAAAAAAAAAAAAAwSuQEAAAAAAAAAAAAAAAAjFoocqqqU1X1VlVdraqtO+z7yarqqtpc3ogAAAAAAAAAAAAAAADAKts3cqqqB5I8n+SJJCeTnK2qk7fY97Ekv5Dk75Y9JAAAAAAAAAAAAAAAALC6FjnJ6bEkV7v77e5+P8mLSZ66xb5fS/LrSf5rifMBAAAAAAAAAAAAAAAAK26RyOmhJO/sub6+u/Z/qupTSY5396U7PaiqzlXVdlVt7+zs/L+HBQAAAAAAAAAAAAAAAFbPIpHTHVXVkSS/leRL++3t7he6e7O7N9fX1z/sqwEAAAAAAAAAAAAAAIAVsEjk9G6S43uuj+2ufdvHkvxIkr+qqmtJPp3kYlVtLmtIAAAAAAAAAAAAAAAAYHUtEjm9muREVT1SVQ8meTrJxW/f7O5vdvfR7t7o7o0kryQ5093bBzIxAAAAAAAAAAAAAAAAsFL2jZy6+4MkzyR5KcmbSS509xtV9VxVnTnoAQEAAAAAAAAAAAAAAIDVtrbIpu6+nOTyTWvP3mbv4x9+LAAAAAAAAAAAAAAAAOCw2PckJwAAAAAAAAAAAAAAAICDJHICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYJXICAAAAAAAAAAAAAAAARomcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAAAYtTY9AAAAAAAAAHdvY+vS9AiHxrXzp6dHAAAAAAAAWFlOcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYJTICQAAAAAAAAAAAAAAABglcgIAAAAAAAAAAAAAAABGiZwAAAAAAAAAAAAAAACAUSInAAAAAAAAAAAAAAAAYNTa9AAAAAAAAABwmG1sXZoe4dC4dv709AgAAAAAAMBtOMkJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABglMgJAAAAAAAAAAAAAAAAGCVyAgAAAAAAAAAAAAAAAEaJnAAAAAAAAAAAAAAAAIBRIicAAAAAAAAAAAAAAABg1EKRU1Wdqqq3qupqVW3d4v4Xq+pKVb1eVX9ZVT+0/FEBAAAAAAAAAAAAAACAVbRv5FRVDyR5PskTSU4mOVtVJ2/a9rUkm939o0n+LMlvLHtQAAAAAAAAAAAAAAAAYDUtcpLTY0mudvfb3f1+kheTPLV3Q3e/3N3f2r18Jcmx5Y4JAAAAAAAAAAAAAAAArKpFIqeHkryz5/r67trtfD7JX3yYoQAAAAAAAAAAAAAAAIDDY22ZD6uqzyXZTPJjt7l/Lsm5JHn44YeX+WoAAAAAAAAAAAAAAADgPrXISU7vJjm+5/rY7tp3qKrPJvlykjPd/d+3elB3v9Ddm929ub6+fjfzAgAAAAAAAAAAAAAAACtmkcjp1SQnquqRqnowydNJLu7dUFWfTPL7uRE4vbf8MQEAAAAAAAAAAAAAAIBVtW/k1N0fJHkmyUtJ3kxyobvfqKrnqurM7rbfTPJ9Sf60qv6+qi7e5nEAAAAAAAAAAAAAAAAA32FtkU3dfTnJ5ZvWnt3z/bNLngsAAAAAAAAAAAAAAAA4JPY9yQkAAAAAAAAAAAAAAADgIImcAAAAAAAAAAAAAAAAgFEiJwAAAAAAAAAAAAAAAGCUyAkAAAAAAAAAAAAAAPjf9u48WrazrBPw703CIFMgBGwkwRsaROIAxDCpaEQbGWwBOyjBgYg0jTZLW43doVkLkbVUQBHROECjRsGWqI12FDQqEAdQkCEhYQjc2IEElVHRaEsLfP3H/m6y77l1zqlzbp27T+37PGvVOlV7/Gr/ar91ald9ewNMSicnAAAAAAAAAAAAAAAAYFI6OQEAAAAAAAAAAAAAAACT0skJAAAAAAAAAAAAAAAAmJROTgAAAAAAAAAAAAAAAMCkdHICAAAAAAAAAAAAAAAAJqWTEwAAAAAAAAAAAAAAADApnZwAAAAAAAAAAAAAAACASenkBAAAAAAAAAAAAAAAAExKJycAAAAAAAAAAAAAAABgUjo5AQAAAAAAAAAAAAAAAJPSyQkAAAAAAAAAAAAAAACYlE5OAAAAAAAAAAAAAAAAwKR0cgIAAAAAAAAAAAAAAAAmpZMTAAAAAAAAAAAAAAAAMCmdnAAAAAAAAAAAAAAAAIBJ6eQEAAAAAAAAAAAAAAAATEonJwAAAAAAAAAAAAAAAGBSJ03dAAAAAAAAAIB1d+DCV0/dhOPGdc97zNRNAAAAAABgD7iSEwAAAAAAAAAAAAAAADApnZwAAAAAAAAAAAAAAACASenkBAAAAAAAAAAAAAAAAExKJycAAAAAAAAAAAAAAABgUjo5AQAAAAAAAAAAAAAAAJM6aeoGAAD724ELXz11E44b1z3vMXu2bDkeGzKcBznOw17lKMNjx744D3uZIwAAwKr5vHjs+LwIAMeG/2+OHd9rzIPvGNeffXEe7Ivrz744D47fMBVXcgIAAAAAAAAAAAAAAAAmpZMTAAAAAAAAAAAAAAAAMCmdnAAAAAAAAAAAAAAAAIBJ6eQEAAAAAAAAAAAAAAAATEonJwAAAAAAAAAAAAAAAGBSOjkBAAAAAAAAAAAAAAAAkzpp6gYAAAAAAAAAAKzCgQtfPXUTjhvXPe8xUzcBAAAAgJlxJScAAAAAAAAAAAAAAABgUjo5AQAAAAAAAAAAAAAAAJPSyQkAAAAAAAAAAAAAAACYlE5OAAAAAAAAAAAAAAAAwKR0cgIAAAAAAAAAAAAAAAAmpZMTAAAAAAAAAAAAAAAAMCmdnAAAAAAAAAAAAAAAAIBJ6eQEAAAAAAAAAAAAAAAATEonJwAAAAAAAAAAAAAAAGBSOjkBAAAAAAAAAAAAAAAAk9LJCQAAAAAAAAAAAAAAAJiUTk4AAAAAAAAAAAAAAADApHRyAgAAAAAAAAAAAAAAACalkxMAAAAAAAAAAAAAAAAwKZ2cAAAAAAAAAAAAAAAAgEnp5AQAAAAAAAAAAAAAAABMSicnAAAAAAAAAAAAAAAAYFI6OQEAAAAAAAAAAAAAAACT0skJAAAAAAAAAAAAAAAAmNRSnZyq6pFVdU1VHayqCxeMv1VVXdLHv6mqDqy6oQAAAAAAAAAAAAAAAMA8bdvJqapOTPIzSR6V5Mwk51XVmRsm+44kf9dau1eSFyV5/qobCgAAAAAAAAAAAAAAAMzTMldyelCSg621v2qt/b8kr0zy2A3TPDbJL/f7v5nkq6uqVtdMAAAAAAAAAAAAAAAAYK6W6eR09yTXjx7f0IctnKa19qkkn0hy51U0EAAAAAAAAAAAAAAAAJi3aq1tPUHVuUke2Vp7an/8rUke3Fp7xmiaq/s0N/TH1/ZpPrphWU9L8rT+8D5JrlnVE4GZODXJR7ediv1MhvMgx/Unw3mQ4zzIcf3JcB7kuP5kOA9yXH8ynAc5zoMc158M50GO60+G8yDH9SfDeZDj+pPhPMhxHuS4/mQ4D3JcfzKcBzmuPxnCkT63tXaXRSNOWmLmDyY5ffT4tD5s0TQ3VNVJSU5O8rGNC2qtvTTJS5dpMRyPquotrbWzp24HuyfDeZDj+pPhPMhxHuS4/mQ4D3JcfzKcBzmuPxnOgxznQY7rT4bzIMf1J8N5kOP6k+E8yHH9yXAe5DgPclx/MpwHOa4/Gc6DHNefDGFnTlhimr9Mcu+qOqOqbpnkiUku3TDNpUme3O+fm+R1bbtLRAEAAAAAAAAAAAAAAABkiSs5tdY+VVXPSHJZkhOT/GJr7Z1V9dwkb2mtXZrkF5K8vKoOJvl4ho5QAAAAAAAAAAAAAAAAANvatpNTkrTWXpPkNRuGPXt0/1+SPGG1TYPj0kunbgBHTYbzIMf1J8N5kOM8yHH9yXAe5Lj+ZDgPclx/MpwHOc6DHNefDOdBjutPhvMgx/Unw3mQ4/qT4TzIcR7kuP5kOA9yXH8ynAc5rj8Zwg5Ua23qNgAAAAAAAAAAAAAAAADHsROmbgAAAAAAAAAAAAAAAABwfNPJiVmqqmdV1Tur6h1VdUVVPbgPP6mqfqSq3teHX1FVzxrN9+k+7J1VdWVVfX9VndDHnVNVF6+gbV9SVVdV1cGq+qmqqgXTVB93sD+Hs0bjntzb/76qevJo+A9X1fVVdeOS7bhzVb2+qm6sqouWaWNVnVJVf9jX/YdVdafdb4nlVNXXVdXbex7vqqr/NBr3LX37HMrrZVV1xz7u8qq6po9/T1VddGhcH3/dCtq21PbYIrPNtvMT+nP6TFWdvYP2nNi31e+Ohp1RVW/q67ikqm7Zh9+qPz7Yxx/Y7XZYsm1y3L4d96mb69IVVfUPVfVftmrjUCoW14pVm0OGfdo7VNUNNap7q9wXq+oXq+rDVXX1Mm08lhn29clx+faoqbtr277IsdTUo2nbsu+LL+htfPeGrNTUyHHBevZlTZXh+tfTvVBVF1fVOStYzjP7Nrimqr52k2l2tB/UFp/jt2jHpplv1saqemQfdrCqLtztNpjS3HIcrev7q6pV1an98ab7W23yGXZdzC3DvpxDtfS6qrpiuzbaFw9bzrY5jqb9qRodH1VTj97cchwtU03d+XL2RYZq6lEvZ5n3xsSlY+oAABN5SURBVK+uqrf1bfxnVXWvPlxNPQpzy3C0ruOmnibzy3E/1tSqelx/TX3+FtNcvrF27HJdB6rq8hUs54xa8JliwXQ72qZV9Yw+7KZ9bIm2/H5V/X2NjsNt1cbNXk9HS45Hl2Ofbyffoaz8dxwy3H2GVfVVdfgx1X+pqsdt1Ub74mHLWSrHPu09anjfu2A0bFY1VYbrX0/7cuV48/C1rKlzzbCqblFVv9z3h3dX1TNH42ZVT/ty5XjzcDV1uXXNKsf9UlNhaa01N7dZ3ZI8NMmfJ7lVf3xqks/p95+X5OIkt+6Pb5/kOaN5bxzdv2uSP0ryQ/3xOUku7vdPTnLCLtv35iQPSVJJfi/JoxZM8+g+rvq0b+rDT0nyV/3vnfr9O/VxD0lyt/Fz2KYdt03y5UmenuSiZdqY5AVJLuz3L0zy/D3O8hZJ/jrJaf3xrZLcp99/ZJK3Jrl7f3xikqeMxl+e5Ox+/5ZJXpjkj0fLvm407ra7bN+222ObzDbbzvdNcp/xc1iyPd+X5H8m+d3RsF9P8sR+/+eTfGe//11Jfr7ff2KSS+S4P3IcbYe/TfK5W7Uxm9QKGW5em5K8uO8nF42GrSzDJF+R5KwkVy/TxmOVoRzV1OM1x9F2UFNXmGGSL03yht6+EzP8733OqjOMmjqLHPt8+66mynD96+mybd7FPBePtuOddrneM5Nc2V9XZyS5dlFbdrofZIvP8Vu0ZWHmm7Wx365Ncs/+Gr4yyZlTZSjHw9ZzepLLkrw/yal92I6PH8lwugxH63thkmdv1Ub74s5z7NOeneTlOfwYr5oqx0XrUFPXPMPRctXUPcgxyXuT3HeU3cWrzjFrXlNleNN61raeynFhu/ZFTU1ySZI/Tf+OfpNpDqsdR7GuA0kuH22/W+xyOQs/UyyT+1bbNMkDehuvO7SPLdGWr07y7zM6DrdVGzd7Pclx2hz7fDv5DmXlv+OQ4dFn2Oc9JcnHk9xmqzbaF3eW42ja30zyG0ku6I9nV1NluP71VI7zqKlzzTDJk5K8st+/Tc/lwKozzD6op3JUU+V4xPom/T/VzW2Z2+QNcHNb9S3JNyT5nQXDb5PkY0luv8W8N254fM8+T2X4AdmL+/DHJHlfkuckuccO2na3JO8ZPT4vyUsWTPeSJOeNHl/T5z1s+o3TLXoOS7Tp/A3/dGzaxkPtGE13zR5neUqSDyf5rAXj/jTJV20x72H/qPQ3+/+T5H798V/2v5/d3+RfkuSBO2zftttjs8yWeS1sfA7btOW0JK9N8vD0DwT9dfvRJCf1xw9Nclm/f1mSh/b7J/XpSo7T5jia5xFJ3rBdG7NJrZDh4tqU5EuSvDKjurcXGWb48LDxB/mTZihHNfV4zHE0j5q64gz76/+tST4rw//Yb8nw4yQ1VY6L2rIva6oM17ueZqgN70nyq0neneELtEMHYK9L8vwkb8twoPW8JFcluTqjg+dJbkzyoiTv7K/Ru/ThL07ypf3+K5K8Lsk3p58sZcn2PTPJM0ePb3pdj4btej/Ihs/xS7Zp4+t2YRvH7Vg0nRyny7Fvn/tl9EVFjuL4kQwn2xcryfVJ7m1fXF2OffiJSV6fDSeB2qMcL4+autY5Rk1d+wz7PGrq3uV4TZIHj+b5kT3M8fLso5oqw/Wvp3Jc75qa5HZJPpjk8zI6FpLh2Mcre56/leRNufkEND+X4ZjIOzP6kVvP+keTXNHHn9Wfx7VJnt6nOT3Jq/r9B2Q4vvPj6Z3LdrDtFn6mWCb3ZbZpdv7Dw3Ny+MmGjumxODkefY7Z4XcoWfHvOGS4mn2xz/O0JL9qX1xtjn3c45L8WIbfbV2waPpV5JgJa6oM17+eynEeNXXOGfbX/+/07XTnDCdaOGUvMoz/Udc+x6ips8hxNM9k/6e6uS17OyEwP3+Q5PSqem9V/WxVfWUffq8kH2it/eOyC2qt/VWGL+Xu2lp7Y2vte/rwV2co5J9Icmm/pOYTFl1GcIO7J7lh9PiGPmzRdNcvmG6z4au0VRs/u7X2N/3+32b40d6eaa19PMmlSd5fVb9WVd9cVYfq1hdk+AJi2WV9OkMv5s/vjx/Y/34ow9kCX5/kh6vq7VX13VV1yhKLXWZ7bJXlMq+FZf1kkv+a5DOjYXdO8vettU8tWMdN7erjP9GnXzk57soTk/zaEm08FjVhFhn29r4wyQUbRu1Vhsu28ZhkmMhxh9TUeeR4iJq64gxba3/el/03/XZZa+3dUVPluNi+rKky3JV9VU8zbNufba3dN8k/ZDiL1CEfa62dleRPMvx47eFJ7p/kgVX1uD7NbZO8pbX2BUn+OMkPJklr7Xtaa2/s978lyQ9kOOnJO6vqp6vqfku0bZltMPV+MOVxhzE5LqGqHpvkg621K5ds47HMUYY787AkH2qtvW+bNtoXb7bstnhGkktH701HzK+myjFRU+eQ4Yiaunc5PjXJa6rqhiTfmuR5G+efeU2V4RL2eT1N5LhT+6WmPjbJ77fW3pvkY1X1JX34dyb5557nD2b4cd0hz2qtnZ3ki5N8ZVV98WjcB1pr989wQpuLk5yb4QzjP5QkrbXrW2vf0O+/vS/jPUleVlV/VlXfXlW33abNW32mGJty3zjWn3vkeBQ57vI7lFX/jkOGe3NM1b64ohyr6nZJ/tuhdY/MrabKcP3raSLHTeffhalq6mwzzHBCiH/K8D3jB5L8eBu+Q51bPU3kuNX821JT55HjBlP+nwpL0cmJ2Wmt3ZjhTeppST6S5JKqOn/jdP0N5oqqur6qTt/Fej7aWntRf5P7oSTPzdCj97jQWmtJ2jFYz1MzXLL0zRn+SfrFjdNU1Rf1LK+tqm/aYnG1yTo+2Vp7ZWvtERn+EfqaJH9dVZ+zg3Yek+2xSFV9XZIPt9beOsX6lyHH5dXQWfLrM1zK+QhTtXEGGX5Xkte01m5YMO6YmvJ1Jsftqak3rWOtczxETd2bDKvqXhmuFnNahgMaD6+qhy273FVSU5dq52Q57veaKsPl7dN6en1r7Q39/iuSfPlo3CX97wOTXN5a+0g/6PqrSb6ij/vMaLqN89+ktfbW1tp/ztD57WCSN1fV963uaRz35LiNqrpNkv+e5NlTt2UTMtyZ83J4h9H9Yq1z7O+rT0jy00e7rDUnx+3XoaZmvTPcQE3du/fG703y6NbaaUl+KclPrGi560KG21iDeprIcaf2S009L8PZt9P/ntfvf0WGHNJae0eSd4zm+caqeluSt2fI4czRuEv736uSvKm19o+ttY8k+WRV3XHjyvv4l7XWvizJf+y3jR132Z4cj85RfYeyouN0MlyBqrpbki/KcPb7Kcw5x+ckeVEbfh82ZzI8OvuhniZyXImJa+qcM3xQkk8n+ZwkZyT5/qq654qWvd/I8eioqZlFjkkmr6mwtJOmbgDshTacSfvyJJdX1VVJnpzk15Pco6pu399sfinJL1XV1Rmu1nSE/ibx6SQf3mT8mUm+PcOlV/84yf/YpmkfzPADtENO68MWTXf6guk+mOHSnePhl2+zzp3aqo0fqqq7tdb+pr/RLdwuq9ZauyrJVVX18gyXfDw/w+Ujz0ry+j7+/lV1UYZLTx6hqk7M8Mb87k3G3zXDWdC+LUNP5Ccl+dA2TVtme2yW2bKvhWV8WZKvr6pHJ7l1kjtU1SsyPJ87VtVJ/cuZ8ToOvcZuqKqTkpyc5GO7XP9S5Li0RyV5WxvO/L9dGzerFXtizTN8aJKHVdV3Zbh07S2r6sYkL87qM9xJG49phokcl6Cm3jx+nXM8RE3dmwwfn+QvDh2wrqrfy5Dry6Omnh85ju37mirDpe3HerrxQPj48T+tYHlJkv46fHSSp2S4QvSz0w+Sb2GZbfCxTLsfbNXGY1lL5bi9f5vhy4wrq+pQO95WVQ/aoo3H4vjRITJcUl/GN+TwMwjaF1eT4wP68g72/eQ2VXWwtXavqKmHyHGgpmbtMxw/BzV1D3KsqrskuV9r7U190CVJfn/D/HOvqTLc3n6vp4kcl7ZfamoNV75+eJIvqqqW4fv7VlU/sMU8Z2Q4cc0DW2t/V1UXZzgGdcgn+9/PjO4ferzwtzJVdSDDbwvOy3Dl7uds0/StPlOMTVnfjuXnHjkeOXyndvMdysp+xyHDhcN36xuT/FZr7V+XaKN9cbBsjg9Ocm5VvSDJHZN8pqr+JclbM5OaKsObrG09TeQ4mm5ta+pxkOGTMlwV51+TfLiq3pDk7AxXbZlFPU3kOJpOTZXjIZP9nwo74UpOzE5V3aeq7j0adP8k72+t/XOSX0hyUVXduk97YpJbbrKcuyT5+SQX9Z7E43FnVdVfJHlZhssPPqC19tTRgeZU1Xs2LrMNl1/8h6p6SA1H/b8tyf9esPpLk3xbDR6S5BN93suSPKKq7lRVd0ryiGzTm7aqHl9VP7rVNDto46UZ3qDT/y5q+8pU1e2q6pzRoPsneX+//6NJfryqxv8kbfajw1v06a/vPbXH406uqt9O8icZ/oF5dGvtMa21V/XOcqmqX6nhi5mNltkeCzPbwWth3Na7V9VrNw5vrT2ztXZaa+1AhstIvq619i39dfv6DJfR3NjGcdvP7fPsyZnX5XjE81iY48iis+Vt1sbNasVKzSHD1to3t9bu0feTC5L8Smvtwj3KcJFJM0zkuOB5qKkzznFETd2b98UPZLiM90m9HV+Z5N1qqhw3Dt/PNVWGRzyPtaunGU5i8tB+/0lJ/mzBNG/OsI1OreHz/3kZTlCSDMfEzt1q/hrOvv3eJP8hyQtba1/YWnt+a+3Dffxmn7cvTfLEqrpVDQfP793bcpNV7wdbvJY2s1kb/zLJvavqjBqu4PXE3HwWs70gx8PbekSOrbWrWmt3ba0d6PX0hiRntdb+Nis8fnQUZHh4W7faF78myXva4Wc6tC+uJsdXt9b+zWg/+ec2dKo4NL+aerPjOkc1df0zHFFT9yjHJH+X5OSq+rz++N/l5hM6HC81VYaHt3Ud62kix41tXYeaem6Sl7fWPre/tk7PcEKah2U4tvKk/ly+MMkX93nukKHT2ieq6rMznKRlV6rqQFX9UZLfTvL3Sb6stfZNrbU/GE3z2qq6+3i+bT5TjK1sm1bVg6rqV5Z9bsf4WJwcjzLHtrvvUFb5Ow4Zrm5fPOyYqn1xdTm21h42+j/kJ5P8SGvtosyrpspw/etpIsc51NRZZ5jhu8aH9+XcNslDMvwWdk719NAy5KimHvc5jkz5fyosr7Xm5jarW4azTL0xybsyXHbwVUlO7eNukeR5SQ5muPzgG5M8K8kt+/hPJ7kiw5m7r8zwhnzCgnXcN8l9t2jDqUmu2WTc2UmuTnJtkouSVB/+9CRP7/cryc/0aa5KcvZo/qf09h9M8u2j4S/I8CXCZ/rf5/ThFyR55iZtuS7Jx5Pc2Oc5c5s23jnJa5O8L8kfJTlljzJ8TYbLLt6+37+m5/KGDdviyX37vKtn+dIkd+vjLu/zvaP//Zkkd1ywrpMz/HNQW7TniiSnLRi+cHv07feyJTLbbDs/vufxyQxneL9sNP1l22y7c5L87ujxPTMclDuY5DeS3KoPv3V/fLCPv6ccp88xyW0z9HQ/eck2blorZHhkhqPpz8/QgXUvMvy1DJeh/dc+73dMmaEc1dTjOceoqXuWYYYz4rwkww8r3pXkJ/YoQzV1BjmOlnlO9kFNleH619O+jgMZDuq+oj///5XkNn3cdenHAPrj83o7rk7y/NHwG5P8RB/+uiR3WbCer0lyhy3asdXn7Wf1bXBNkkdtfA3udj/I5p/jN3stLcx8mzY+OsMP9a5N8qxV5yfHnee4oU03bZvs4viRDKfLMMnF6cf+lmyjfXEHOW6Y/sbRfTVVjlttt5u2TdTUtcowaupevzc+vrftygyff+656hyzT2uqDNe/nspxfWtqhh9UPXLDsO9O8nMZTj7zyp7nq5K86dBrq7f/vRmOUbwqyfkLXpfn5/Dj0Ie9Dvqw05M8aIv2nZDhBDmftWDcZp8pvj7Jc3e7TfvzvyHJp5L8dW4+JnRukpds0s4/TfKRJP+3z/u127Rxpcfi5LiaHEfzbnzOe/47DhmubF88kOGs9ycs2Ub74g5zHE3/nCQX7FGOk9VUGa5/PZXjPGrq3DPMcEWe38jwW9l3JfmBPcrQ/6gzyHE078bnrKauUY6Z+P9UN7ed3A4VE2CFqurrMhTzn9oHbXlFku9trX1k6raso6q6Q5JfaK09YR+05RlJPtBa28uzTM6SHNefDOdBjvMgx/Unw3mQ4/qT4e5V1YEMnea+8CiWcWNr7XZH2Y598Xl7P72WdkKOR7Rj7XKU4RHtWLsMEzkuaIccj64dctwlGR7RjrXLMJHjgnasXY4yPKIda5dhIscF7VjLHPebfjbwp7TWvm8ftOXHMpzB/B3bTsxh5Lj+ZDgPclx/MpwHOa4/Gc6DHOdBjrB6OjkBAAAAsDb2yw/WODpyXH8ynAc5zoMc158M50GO60+G8yBHAAAAAFhvOjkBAAAAAAAAAAAAAAAAkzph6gYAAAAAAAAAAAAAAAAAxzednAAAAAAAAAAAAAAAAIBJ6eQEAAAAAAAAAAAAAAAATEonJwAAAAAAAAAAAAAAAGBSOjkBAAAAAAAAAAAAAAAAk9LJCQAAAAAAAAAAAAAAAJjU/weoW/QjzLF68QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f=plt.figure()\n",
        "f.set_figheight(6)\n",
        "f.set_figwidth(60)\n",
        "plt.bar(mod,ac)"
      ],
      "id": "8a6b802f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ab45f07"
      },
      "source": [
        "### FROM the plot for model-3 we can see that \n",
        "### FOR SGD           \n",
        "the optimal batch size is 100-400 which is yileding good accuracy\n",
        "the optimal learning rate is 0.001 since for other values the accuracy is quite depressing\n",
        "### FOR rmsprop\n",
        "there is no much impact of batch size. Almost all batch sizes are producing simmilar kind of accuracy\n",
        "the optimal learning rate is 0.001-0.4 since for other values the accuracy is quite depressing\n",
        "### FOR Adam\n",
        "there is no much impact of batch size. Almost all batch sizes are producing simmilar kind of accuracy\n",
        "the optimal learning rate is 0.001 since for other values the accuracy is quite depressing\n",
        "\n",
        "### OVERALL\n",
        "Finally we can see the with lower value of learning rate, all model-3 are performoing good\n",
        "There is no much impact of batch size for Adam and rmsprop but the SGD is being affected by batch size"
      ],
      "id": "1ab45f07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867f2daf"
      },
      "source": [
        "### max accuracy producing parameters for second model"
      ],
      "id": "867f2daf"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313c9281",
        "outputId": "212f2bbb-4c47-467a-eed1-a3c78c3685ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam'>, 0.001, 700\n",
            "0.9830999970436\n"
          ]
        }
      ],
      "source": [
        "print(mod[ac.index(max(ac))])\n",
        "print(max(ac))"
      ],
      "id": "313c9281"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aede68b2"
      },
      "source": [
        "### OVRALL REPORT"
      ],
      "id": "aede68b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2372f979"
      },
      "source": [
        "The highest accurate model( hour glass shapped model,<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 700) of all 3 models yeild the accuracy of 98.3799%              \n",
        "The hour glass shapped model yeilds good accuracy with parameters Adam and learning rate 0.001 and batch size 700            \n",
        "Regular CNN model yeilds good accuracy with parameters Adam and learning rate 0.001 and batch size 400             \n",
        "Inverted CNN model yeilds good accuracy with parameters rmsprop and learning rate 0.001 and batch size 100"
      ],
      "id": "2372f979"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6e12df3",
        "outputId": "b7b6c033-715a-45a0-ba6d-09d1d0996ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9822999835014343\n"
          ]
        }
      ],
      "source": [
        "maxval=0\n",
        "for i,j in acc.items():\n",
        "    if(j>maxval):\n",
        "        maxval=j\n",
        "        z=i\n",
        "print(maxval)\n",
        "mx_acc.append(z)    "
      ],
      "id": "c6e12df3"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a8f45b",
        "outputId": "7f9ff303-fab4-4cac-bc01-600d819bf80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400), (<class 'keras.optimizers.optimizer_v2.adam.Adam'>, 0.001, 400), (<class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 0.001, 400)]\n"
          ]
        }
      ],
      "source": [
        "print(mx_acc)"
      ],
      "id": "c5a8f45b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4dbc449"
      },
      "source": [
        "### 2."
      ],
      "id": "e4dbc449"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12748a8b"
      },
      "source": [
        "##### 3. Different parameters"
      ],
      "id": "12748a8b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2278bdb9"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n"
      ],
      "id": "2278bdb9"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1df4d46e",
        "outputId": "d1fc2f8d-89b9-46bb-ddff-7bbb42433d9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_train.shape"
      ],
      "id": "1df4d46e"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d2af5a4b"
      },
      "outputs": [],
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "id": "d2af5a4b"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0ab70c5b"
      },
      "outputs": [],
      "source": [
        "X_train= X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255 "
      ],
      "id": "0ab70c5b"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20f86f3b",
        "outputId": "cb6df642-f95e-4966-a1c3-1b68fe74b6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "500/500 [==============================] - 44s 86ms/step - loss: 1.6446 - accuracy: 0.4129\n",
            "Epoch 2/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.4070 - accuracy: 0.5029\n",
            "Epoch 3/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 1.3285 - accuracy: 0.5364\n",
            "Epoch 4/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.2817 - accuracy: 0.5524\n",
            "Epoch 5/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.2580 - accuracy: 0.5621\n",
            "Epoch 6/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.2386 - accuracy: 0.5691\n",
            "Epoch 7/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.2235 - accuracy: 0.5741\n",
            "Epoch 8/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 1.2077 - accuracy: 0.5783\n",
            "Epoch 9/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1982 - accuracy: 0.5827\n",
            "Epoch 10/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1865 - accuracy: 0.5878\n",
            "Epoch 11/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.1786 - accuracy: 0.5877\n",
            "Epoch 12/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.1693 - accuracy: 0.5952\n",
            "Epoch 13/25\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 1.1584 - accuracy: 0.5973\n",
            "Epoch 14/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1569 - accuracy: 0.5993\n",
            "Epoch 15/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1505 - accuracy: 0.6005\n",
            "Epoch 16/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1440 - accuracy: 0.6024\n",
            "Epoch 17/25\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 1.1373 - accuracy: 0.6054\n",
            "Epoch 18/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1336 - accuracy: 0.6072\n",
            "Epoch 19/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1271 - accuracy: 0.6096\n",
            "Epoch 20/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 1.1218 - accuracy: 0.6119\n",
            "Epoch 21/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.1151 - accuracy: 0.6118\n",
            "Epoch 22/25\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 1.1168 - accuracy: 0.6137\n",
            "Epoch 23/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.1088 - accuracy: 0.6161\n",
            "Epoch 24/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.1038 - accuracy: 0.6184\n",
            "Epoch 25/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1.1000 - accuracy: 0.6202\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.1841 - accuracy: 0.5909\n",
            "Epoch 1/25\n",
            "125/125 [==============================] - 37s 295ms/step - loss: 1.8233 - accuracy: 0.3557\n",
            "Epoch 2/25\n",
            "125/125 [==============================] - 37s 294ms/step - loss: 1.5726 - accuracy: 0.4446\n",
            "Epoch 3/25\n",
            "125/125 [==============================] - 37s 294ms/step - loss: 1.4752 - accuracy: 0.4781\n",
            "Epoch 4/25\n",
            "125/125 [==============================] - 37s 293ms/step - loss: 1.4175 - accuracy: 0.5004\n",
            "Epoch 5/25\n",
            "125/125 [==============================] - 37s 294ms/step - loss: 1.3723 - accuracy: 0.5198\n",
            "Epoch 6/25\n",
            "125/125 [==============================] - 38s 305ms/step - loss: 1.3361 - accuracy: 0.5321\n",
            "Epoch 7/25\n",
            "125/125 [==============================] - 36s 289ms/step - loss: 1.3160 - accuracy: 0.5394\n",
            "Epoch 8/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 1.2954 - accuracy: 0.5470\n",
            "Epoch 9/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 1.2790 - accuracy: 0.5530\n",
            "Epoch 10/25\n",
            "125/125 [==============================] - 36s 289ms/step - loss: 1.2621 - accuracy: 0.5589\n",
            "Epoch 11/25\n",
            "125/125 [==============================] - 37s 298ms/step - loss: 1.2626 - accuracy: 0.5591\n",
            "Epoch 12/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 1.2448 - accuracy: 0.5653\n",
            "Epoch 13/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 1.2357 - accuracy: 0.5687\n",
            "Epoch 14/25\n",
            "125/125 [==============================] - 36s 289ms/step - loss: 1.2247 - accuracy: 0.5736\n",
            "Epoch 15/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 1.2120 - accuracy: 0.5792\n",
            "Epoch 16/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 1.2116 - accuracy: 0.5778\n",
            "Epoch 17/25\n",
            "125/125 [==============================] - 37s 298ms/step - loss: 1.2057 - accuracy: 0.5792\n",
            "Epoch 18/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 1.1976 - accuracy: 0.5821\n",
            "Epoch 19/25\n",
            "125/125 [==============================] - 36s 289ms/step - loss: 1.1881 - accuracy: 0.5863\n",
            "Epoch 20/25\n",
            "125/125 [==============================] - 36s 286ms/step - loss: 1.1807 - accuracy: 0.5911\n",
            "Epoch 21/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 1.1782 - accuracy: 0.5904\n",
            "Epoch 22/25\n",
            "125/125 [==============================] - 37s 297ms/step - loss: 1.1724 - accuracy: 0.5932\n",
            "Epoch 23/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 1.1643 - accuracy: 0.5964\n",
            "Epoch 24/25\n",
            "125/125 [==============================] - 36s 286ms/step - loss: 1.1591 - accuracy: 0.5988\n",
            "Epoch 25/25\n",
            "125/125 [==============================] - 36s 286ms/step - loss: 1.1523 - accuracy: 0.6011\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.2202 - accuracy: 0.5769\n",
            "Epoch 1/25\n",
            "72/72 [==============================] - 38s 515ms/step - loss: 1.8242 - accuracy: 0.3536\n",
            "Epoch 2/25\n",
            "72/72 [==============================] - 37s 516ms/step - loss: 1.5316 - accuracy: 0.4583\n",
            "Epoch 3/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 1.4329 - accuracy: 0.4956\n",
            "Epoch 4/25\n",
            "72/72 [==============================] - 36s 499ms/step - loss: 1.3748 - accuracy: 0.5167\n",
            "Epoch 5/25\n",
            "72/72 [==============================] - 36s 498ms/step - loss: 1.3416 - accuracy: 0.5313\n",
            "Epoch 6/25\n",
            "72/72 [==============================] - 36s 498ms/step - loss: 1.3030 - accuracy: 0.5460\n",
            "Epoch 7/25\n",
            "72/72 [==============================] - 36s 498ms/step - loss: 1.2780 - accuracy: 0.5530\n",
            "Epoch 8/25\n",
            "72/72 [==============================] - 37s 509ms/step - loss: 1.2613 - accuracy: 0.5597\n",
            "Epoch 9/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 1.2470 - accuracy: 0.5627\n",
            "Epoch 10/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 1.2351 - accuracy: 0.5697\n",
            "Epoch 11/25\n",
            "72/72 [==============================] - 36s 495ms/step - loss: 1.2105 - accuracy: 0.5809\n",
            "Epoch 12/25\n",
            "72/72 [==============================] - 35s 490ms/step - loss: 1.2003 - accuracy: 0.5832\n",
            "Epoch 13/25\n",
            "72/72 [==============================] - 35s 493ms/step - loss: 1.1970 - accuracy: 0.5842\n",
            "Epoch 14/25\n",
            "72/72 [==============================] - 37s 517ms/step - loss: 1.1918 - accuracy: 0.5855\n",
            "Epoch 15/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 1.1855 - accuracy: 0.5907\n",
            "Epoch 16/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 1.1735 - accuracy: 0.5920\n",
            "Epoch 17/25\n",
            "72/72 [==============================] - 35s 491ms/step - loss: 1.1661 - accuracy: 0.5962\n",
            "Epoch 18/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 1.1605 - accuracy: 0.5972\n",
            "Epoch 19/25\n",
            "72/72 [==============================] - 37s 507ms/step - loss: 1.1604 - accuracy: 0.5967\n",
            "Epoch 20/25\n",
            "72/72 [==============================] - 35s 490ms/step - loss: 1.1527 - accuracy: 0.6017\n",
            "Epoch 21/25\n",
            "72/72 [==============================] - 35s 490ms/step - loss: 1.1421 - accuracy: 0.6051\n",
            "Epoch 22/25\n",
            "72/72 [==============================] - 35s 491ms/step - loss: 1.1425 - accuracy: 0.6040\n",
            "Epoch 23/25\n",
            "72/72 [==============================] - 35s 491ms/step - loss: 1.1342 - accuracy: 0.6092\n",
            "Epoch 24/25\n",
            "72/72 [==============================] - 36s 495ms/step - loss: 1.1370 - accuracy: 0.6066\n",
            "Epoch 25/25\n",
            "72/72 [==============================] - 37s 509ms/step - loss: 1.1307 - accuracy: 0.6108\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.2127 - accuracy: 0.5767\n",
            "Epoch 1/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 4730554.5000 - accuracy: 0.2369\n",
            "Epoch 2/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3758.7598 - accuracy: 0.3068\n",
            "Epoch 3/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 1777.4934 - accuracy: 0.3251\n",
            "Epoch 4/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 976.0670 - accuracy: 0.3401\n",
            "Epoch 5/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 542.8415 - accuracy: 0.3520\n",
            "Epoch 6/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 324.2291 - accuracy: 0.3587\n",
            "Epoch 7/25\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 24322807808.0000 - accuracy: 0.3357\n",
            "Epoch 8/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 9780748288.0000 - accuracy: 0.1874\n",
            "Epoch 9/25\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 21583646.0000 - accuracy: 0.2220\n",
            "Epoch 10/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 8044601.5000 - accuracy: 0.2364\n",
            "Epoch 11/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 4747518.5000 - accuracy: 0.2436\n",
            "Epoch 12/25\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3770949.7500 - accuracy: 0.2439\n",
            "Epoch 13/25\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3384017.2500 - accuracy: 0.2430\n",
            "Epoch 14/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 7588177.5000 - accuracy: 0.2470\n",
            "Epoch 15/25\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 54317132.0000 - accuracy: 0.2412\n",
            "Epoch 16/25\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 1280870.0000 - accuracy: 0.2786\n",
            "Epoch 17/25\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3260284.0000 - accuracy: 0.2739\n",
            "Epoch 18/25\n",
            "500/500 [==============================] - 40s 81ms/step - loss: 297858688.0000 - accuracy: 0.1986\n",
            "Epoch 19/25\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 1093787.1250 - accuracy: 0.2362\n",
            "Epoch 20/25\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 526375.0625 - accuracy: 0.2557\n",
            "Epoch 21/25\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 378814.5312 - accuracy: 0.2633\n",
            "Epoch 22/25\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 336594.4688 - accuracy: 0.2609\n",
            "Epoch 23/25\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 540236.1250 - accuracy: 0.2497\n",
            "Epoch 24/25\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 1920770.3750 - accuracy: 0.2535\n",
            "Epoch 25/25\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 76944.8516 - accuracy: 0.3030\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 77064.0703 - accuracy: 0.2889\n",
            "Epoch 1/25\n",
            "125/125 [==============================] - 35s 276ms/step - loss: 24257536.0000 - accuracy: 0.1211\n",
            "Epoch 2/25\n",
            "125/125 [==============================] - 35s 281ms/step - loss: 37117.9844 - accuracy: 0.2308\n",
            "Epoch 3/25\n",
            "125/125 [==============================] - 35s 284ms/step - loss: 8256.8447 - accuracy: 0.2514\n",
            "Epoch 4/25\n",
            "125/125 [==============================] - 35s 283ms/step - loss: 4732.3643 - accuracy: 0.2524\n",
            "Epoch 5/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 3235.2156 - accuracy: 0.2540\n",
            "Epoch 6/25\n",
            "125/125 [==============================] - 35s 282ms/step - loss: 2463.1660 - accuracy: 0.2585\n",
            "Epoch 7/25\n",
            "125/125 [==============================] - 35s 278ms/step - loss: 1906.3713 - accuracy: 0.2696\n",
            "Epoch 8/25\n",
            "125/125 [==============================] - 35s 282ms/step - loss: 1677.0575 - accuracy: 0.2665\n",
            "Epoch 9/25\n",
            "125/125 [==============================] - 35s 281ms/step - loss: 1326.3108 - accuracy: 0.2776\n",
            "Epoch 10/25\n",
            "125/125 [==============================] - 36s 289ms/step - loss: 1106.5865 - accuracy: 0.2864\n",
            "Epoch 11/25\n",
            "125/125 [==============================] - 35s 283ms/step - loss: 973.8697 - accuracy: 0.2849\n",
            "Epoch 12/25\n",
            "125/125 [==============================] - 36s 284ms/step - loss: 828.0680 - accuracy: 0.2899\n",
            "Epoch 13/25\n",
            "125/125 [==============================] - 35s 283ms/step - loss: 763.0044 - accuracy: 0.2917\n",
            "Epoch 14/25\n",
            "125/125 [==============================] - 35s 282ms/step - loss: 620.9910 - accuracy: 0.2983\n",
            "Epoch 15/25\n",
            "125/125 [==============================] - 35s 284ms/step - loss: 513.1913 - accuracy: 0.3062\n",
            "Epoch 16/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 431.9349 - accuracy: 0.3070\n",
            "Epoch 17/25\n",
            "125/125 [==============================] - 35s 283ms/step - loss: 393.5095 - accuracy: 0.3068\n",
            "Epoch 18/25\n",
            "125/125 [==============================] - 35s 281ms/step - loss: 343.2387 - accuracy: 0.3133\n",
            "Epoch 19/25\n",
            "125/125 [==============================] - 35s 284ms/step - loss: 306.4110 - accuracy: 0.3141\n",
            "Epoch 20/25\n",
            "125/125 [==============================] - 35s 282ms/step - loss: 237.4788 - accuracy: 0.3275\n",
            "Epoch 21/25\n",
            "125/125 [==============================] - 35s 283ms/step - loss: 257.4240 - accuracy: 0.3132\n",
            "Epoch 22/25\n",
            "125/125 [==============================] - 36s 291ms/step - loss: 391.7218 - accuracy: 0.3081\n",
            "Epoch 23/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 178.3033 - accuracy: 0.3310\n",
            "Epoch 24/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 168.3242 - accuracy: 0.3204\n",
            "Epoch 25/25\n",
            "125/125 [==============================] - 37s 292ms/step - loss: 134.8163 - accuracy: 0.3382\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 113.5229 - accuracy: 0.3469\n",
            "Epoch 1/25\n",
            "72/72 [==============================] - 35s 474ms/step - loss: 49211532.0000 - accuracy: 0.1010\n",
            "Epoch 2/25\n",
            "72/72 [==============================] - 35s 493ms/step - loss: 872642.3750 - accuracy: 0.1267\n",
            "Epoch 3/25\n",
            "72/72 [==============================] - 35s 479ms/step - loss: 27483.7988 - accuracy: 0.1715\n",
            "Epoch 4/25\n",
            "72/72 [==============================] - 35s 482ms/step - loss: 9776.6104 - accuracy: 0.2036\n",
            "Epoch 5/25\n",
            "72/72 [==============================] - 35s 481ms/step - loss: 5592.7534 - accuracy: 0.2232\n",
            "Epoch 6/25\n",
            "72/72 [==============================] - 35s 481ms/step - loss: 4771.4731 - accuracy: 0.2206\n",
            "Epoch 7/25\n",
            "72/72 [==============================] - 35s 480ms/step - loss: 2600.1084 - accuracy: 0.2461\n",
            "Epoch 8/25\n",
            "72/72 [==============================] - 36s 500ms/step - loss: 2118.7952 - accuracy: 0.2501\n",
            "Epoch 9/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 1731.9502 - accuracy: 0.2559\n",
            "Epoch 10/25\n",
            "72/72 [==============================] - 36s 497ms/step - loss: 1666.5837 - accuracy: 0.2525\n",
            "Epoch 11/25\n",
            "72/72 [==============================] - 36s 498ms/step - loss: 1691.3060 - accuracy: 0.2518\n",
            "Epoch 12/25\n",
            "72/72 [==============================] - 36s 495ms/step - loss: 1172.8091 - accuracy: 0.2709\n",
            "Epoch 13/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 1407.4219 - accuracy: 0.2506\n",
            "Epoch 14/25\n",
            "72/72 [==============================] - 37s 510ms/step - loss: 914.3800 - accuracy: 0.2748\n",
            "Epoch 15/25\n",
            "72/72 [==============================] - 36s 493ms/step - loss: 907.2295 - accuracy: 0.2714\n",
            "Epoch 16/25\n",
            "72/72 [==============================] - 36s 499ms/step - loss: 866.0790 - accuracy: 0.2672\n",
            "Epoch 17/25\n",
            "72/72 [==============================] - 36s 497ms/step - loss: 764.4039 - accuracy: 0.2690\n",
            "Epoch 18/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 626.0316 - accuracy: 0.2815\n",
            "Epoch 19/25\n",
            "72/72 [==============================] - 36s 498ms/step - loss: 588.0056 - accuracy: 0.2802\n",
            "Epoch 20/25\n",
            "72/72 [==============================] - 37s 503ms/step - loss: 588.7319 - accuracy: 0.2816\n",
            "Epoch 21/25\n",
            "72/72 [==============================] - 35s 488ms/step - loss: 484.5150 - accuracy: 0.2874\n",
            "Epoch 22/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 608.1368 - accuracy: 0.2643\n",
            "Epoch 23/25\n",
            "72/72 [==============================] - 36s 497ms/step - loss: 513.3930 - accuracy: 0.2746\n",
            "Epoch 24/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 492.2216 - accuracy: 0.2762\n",
            "Epoch 25/25\n",
            "72/72 [==============================] - 36s 507ms/step - loss: 508.5912 - accuracy: 0.2655\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 414.1111 - accuracy: 0.2482\n",
            "Epoch 1/25\n",
            "500/500 [==============================] - 43s 84ms/step - loss: 184507376.0000 - accuracy: 0.2017\n",
            "Epoch 2/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 57373.5508 - accuracy: 0.2531\n",
            "Epoch 3/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 3284680.5000 - accuracy: 0.2061\n",
            "Epoch 4/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 12215.8525 - accuracy: 0.2457\n",
            "Epoch 5/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 6379.7212 - accuracy: 0.2508\n",
            "Epoch 6/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 4259.3066 - accuracy: 0.2541\n",
            "Epoch 7/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 3498.2388 - accuracy: 0.2513\n",
            "Epoch 8/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 2847.2175 - accuracy: 0.2525\n",
            "Epoch 9/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 24281.0137 - accuracy: 0.2124\n",
            "Epoch 10/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 3080.3787 - accuracy: 0.2808\n",
            "Epoch 11/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 585.8990 - accuracy: 0.2864\n",
            "Epoch 12/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 449.2827 - accuracy: 0.2820\n",
            "Epoch 13/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 473.8966 - accuracy: 0.2723\n",
            "Epoch 14/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 889548832768.0000 - accuracy: 0.2191\n",
            "Epoch 15/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 2984760576.0000 - accuracy: 0.1597\n",
            "Epoch 16/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 800192000.0000 - accuracy: 0.1701\n",
            "Epoch 17/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 350380224.0000 - accuracy: 0.1794\n",
            "Epoch 18/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 152502928.0000 - accuracy: 0.1996\n",
            "Epoch 19/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 83724456.0000 - accuracy: 0.2034\n",
            "Epoch 20/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 47308992.0000 - accuracy: 0.2100\n",
            "Epoch 21/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 27618470.0000 - accuracy: 0.2080\n",
            "Epoch 22/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 19551732.0000 - accuracy: 0.1984\n",
            "Epoch 23/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 16827412.0000 - accuracy: 0.1909\n",
            "Epoch 24/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 17637960.0000 - accuracy: 0.1829\n",
            "Epoch 25/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 3597844992.0000 - accuracy: 0.1646\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 30616712.0000 - accuracy: 0.2141\n",
            "Epoch 1/25\n",
            "125/125 [==============================] - 37s 287ms/step - loss: 293710016.0000 - accuracy: 0.1178\n",
            "Epoch 2/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 226117.7969 - accuracy: 0.1814\n",
            "Epoch 3/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 91141.4688 - accuracy: 0.2016\n",
            "Epoch 4/25\n",
            "125/125 [==============================] - 37s 294ms/step - loss: 73691.7422 - accuracy: 0.2019\n",
            "Epoch 5/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 53387.0859 - accuracy: 0.2132\n",
            "Epoch 6/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 37215.5117 - accuracy: 0.2262\n",
            "Epoch 7/25\n",
            "125/125 [==============================] - 36s 289ms/step - loss: 60731.4531 - accuracy: 0.1935\n",
            "Epoch 8/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 78708.4062 - accuracy: 0.1829\n",
            "Epoch 9/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 313939.7500 - accuracy: 0.1400\n",
            "Epoch 10/25\n",
            "125/125 [==============================] - 37s 298ms/step - loss: 11354.5283 - accuracy: 0.2564\n",
            "Epoch 11/25\n",
            "125/125 [==============================] - 36s 288ms/step - loss: 5027.1758 - accuracy: 0.2933\n",
            "Epoch 12/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 3964.4707 - accuracy: 0.2814\n",
            "Epoch 13/25\n",
            "125/125 [==============================] - 36s 287ms/step - loss: 3549.2812 - accuracy: 0.2810\n",
            "Epoch 14/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 3010.1785 - accuracy: 0.2765\n",
            "Epoch 15/25\n",
            "125/125 [==============================] - 36s 291ms/step - loss: 2924.5024 - accuracy: 0.2689\n",
            "Epoch 16/25\n",
            "125/125 [==============================] - 37s 296ms/step - loss: 2636.6150 - accuracy: 0.2646\n",
            "Epoch 17/25\n",
            "125/125 [==============================] - 37s 292ms/step - loss: 2523.1948 - accuracy: 0.2592\n",
            "Epoch 18/25\n",
            "125/125 [==============================] - 36s 291ms/step - loss: 2251.8191 - accuracy: 0.2531\n",
            "Epoch 19/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 2099.3040 - accuracy: 0.2550\n",
            "Epoch 20/25\n",
            "125/125 [==============================] - 36s 292ms/step - loss: 12756.8887 - accuracy: 0.1604\n",
            "Epoch 21/25\n",
            "125/125 [==============================] - 36s 292ms/step - loss: 9601.3750 - accuracy: 0.2279\n",
            "Epoch 22/25\n",
            "125/125 [==============================] - 37s 297ms/step - loss: 1026.3304 - accuracy: 0.3040\n",
            "Epoch 23/25\n",
            "125/125 [==============================] - 37s 294ms/step - loss: 784.0624 - accuracy: 0.2954\n",
            "Epoch 24/25\n",
            "125/125 [==============================] - 36s 290ms/step - loss: 630.1339 - accuracy: 0.2991\n",
            "Epoch 25/25\n",
            "125/125 [==============================] - 36s 291ms/step - loss: 800.5029 - accuracy: 0.2708\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 723.5792 - accuracy: 0.2947\n",
            "Epoch 1/25\n",
            "72/72 [==============================] - 36s 490ms/step - loss: 488669856.0000 - accuracy: 0.0988\n",
            "Epoch 2/25\n",
            "72/72 [==============================] - 36s 504ms/step - loss: 1844627.8750 - accuracy: 0.1491\n",
            "Epoch 3/25\n",
            "72/72 [==============================] - 35s 492ms/step - loss: 180836.3594 - accuracy: 0.1993\n",
            "Epoch 4/25\n",
            "72/72 [==============================] - 36s 493ms/step - loss: 71055.8984 - accuracy: 0.2167\n",
            "Epoch 5/25\n",
            "72/72 [==============================] - 35s 490ms/step - loss: 50120.3164 - accuracy: 0.2300\n",
            "Epoch 6/25\n",
            "72/72 [==============================] - 35s 490ms/step - loss: 34856.3398 - accuracy: 0.2490\n",
            "Epoch 7/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 30105.5820 - accuracy: 0.2506\n",
            "Epoch 8/25\n",
            "72/72 [==============================] - 36s 506ms/step - loss: 23446.0957 - accuracy: 0.2569\n",
            "Epoch 9/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 19530.6562 - accuracy: 0.2621\n",
            "Epoch 10/25\n",
            "72/72 [==============================] - 35s 488ms/step - loss: 16286.4619 - accuracy: 0.2635\n",
            "Epoch 11/25\n",
            "72/72 [==============================] - 35s 489ms/step - loss: 16718.7422 - accuracy: 0.2611\n",
            "Epoch 12/25\n",
            "72/72 [==============================] - 35s 488ms/step - loss: 12388.0371 - accuracy: 0.2761\n",
            "Epoch 13/25\n",
            "72/72 [==============================] - 35s 493ms/step - loss: 10968.0098 - accuracy: 0.2814\n",
            "Epoch 14/25\n",
            "72/72 [==============================] - 36s 504ms/step - loss: 9669.9873 - accuracy: 0.2751\n",
            "Epoch 15/25\n",
            "72/72 [==============================] - 35s 492ms/step - loss: 12329.6533 - accuracy: 0.2573\n",
            "Epoch 16/25\n",
            "72/72 [==============================] - 35s 493ms/step - loss: 8012.3398 - accuracy: 0.2905\n",
            "Epoch 17/25\n",
            "72/72 [==============================] - 36s 494ms/step - loss: 5704.5938 - accuracy: 0.3062\n",
            "Epoch 18/25\n",
            "72/72 [==============================] - 36s 496ms/step - loss: 4681.6904 - accuracy: 0.3107\n",
            "Epoch 19/25\n",
            "72/72 [==============================] - 35s 488ms/step - loss: 5480.8833 - accuracy: 0.2887\n",
            "Epoch 20/25\n",
            "72/72 [==============================] - 36s 503ms/step - loss: 7209.8662 - accuracy: 0.2738\n",
            "Epoch 21/25\n",
            "72/72 [==============================] - 35s 488ms/step - loss: 1036960399360.0000 - accuracy: 0.1123\n",
            "Epoch 22/25\n",
            "72/72 [==============================] - 35s 487ms/step - loss: 249540263936.0000 - accuracy: 0.1066\n",
            "Epoch 23/25\n",
            "72/72 [==============================] - 35s 492ms/step - loss: 1660191744.0000 - accuracy: 0.1847\n",
            "Epoch 24/25\n",
            "72/72 [==============================] - 35s 486ms/step - loss: 553689344.0000 - accuracy: 0.1996\n",
            "Epoch 25/25\n",
            "72/72 [==============================] - 35s 490ms/step - loss: 444196480.0000 - accuracy: 0.1973\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 227682208.0000 - accuracy: 0.2390\n"
          ]
        }
      ],
      "source": [
        "sc=[]\n",
        "a=[]\n",
        "for i in np.arange(0.001,1,0.4):\n",
        "    for j in np.arange(100,800,300):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(6, kernel_size=(5,5), strides=(1, 1), input_shape=(32,32,3)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "        model.add(Conv2D(16, kernel_size=(5,5), strides=(1,1)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "        model.add(Conv2D(120, kernel_size=(5,5)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(84))\n",
        "        model.add(Dense(10))\n",
        "        model.add(Activation('softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=i), metrics=['accuracy'])\n",
        "        model.fit(X_train,Y_train, batch_size=j, epochs=25)\n",
        "        score = model.evaluate(X_test, Y_test)\n",
        "        sc.append(score)\n",
        "        a.append((i,j))"
      ],
      "id": "20f86f3b"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "301645cb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "id": "301645cb"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1bdb048",
        "outputId": "3a9c1dc3-ece0-4980-a61b-401a4410864c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.001, 100), (0.001, 400), (0.001, 700), (0.401, 100), (0.401, 400), (0.401, 700), (0.801, 100), (0.801, 400), (0.801, 700)]\n"
          ]
        }
      ],
      "source": [
        "print(a)"
      ],
      "id": "d1bdb048"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "978196eb"
      },
      "outputs": [],
      "source": [
        "l=[sc[i][1] for i in range(len(sc))]"
      ],
      "id": "978196eb"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfbc04a",
        "outputId": "4dab08c6-a909-40b0-d643-5662033cf8cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5909000039100647,\n",
              " 0.5769000053405762,\n",
              " 0.57669997215271,\n",
              " 0.2888999879360199,\n",
              " 0.34689998626708984,\n",
              " 0.24819999933242798,\n",
              " 0.21410000324249268,\n",
              " 0.2946999967098236,\n",
              " 0.23899999260902405]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "l"
      ],
      "id": "ecfbc04a"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "e7b38ad7"
      },
      "outputs": [],
      "source": [
        "l1=[str(i) for i in a]"
      ],
      "id": "e7b38ad7"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "f3bac0c1",
        "outputId": "4415fada-e44a-4e53-e246-b86e13f982fb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "f = plt.figure()\n",
        "plt.bar(l1,l)\n",
        "f.set_figheight(6)\n",
        "f.set_figwidth(20)"
      ],
      "id": "f3bac0c1"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c478fc52",
        "outputId": "558b931c-9f90-4666-85b7-0b46d25f0711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.001, 100)\n"
          ]
        }
      ],
      "source": [
        "print(l1[l.index(max(l))])"
      ],
      "id": "c478fc52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112d4140"
      },
      "source": [
        "###  The above parameters that is learning rate of 0.01 and batch size of 100 is yeilding best accuracy for given data"
      ],
      "id": "112d4140"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddfda4c4"
      },
      "source": [
        "#### 1.Various learning rate"
      ],
      "id": "ddfda4c4"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "140ceeb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18258165-9464-4b25-ba2f-629a47353645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "196/196 [==============================] - 37s 184ms/step - loss: 1.7125 - accuracy: 0.3873\n",
            "Epoch 2/25\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.4627 - accuracy: 0.4836\n",
            "Epoch 3/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.3603 - accuracy: 0.5211\n",
            "Epoch 4/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 1.3054 - accuracy: 0.5434\n",
            "Epoch 5/25\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.2697 - accuracy: 0.5559\n",
            "Epoch 6/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.2489 - accuracy: 0.5639\n",
            "Epoch 7/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.2239 - accuracy: 0.5735\n",
            "Epoch 8/25\n",
            "196/196 [==============================] - 44s 226ms/step - loss: 1.2079 - accuracy: 0.5784\n",
            "Epoch 9/25\n",
            "196/196 [==============================] - 42s 213ms/step - loss: 1.1926 - accuracy: 0.5872\n",
            "Epoch 10/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 1.1786 - accuracy: 0.5885\n",
            "Epoch 11/25\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 1.1788 - accuracy: 0.5900\n",
            "Epoch 12/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 1.1625 - accuracy: 0.5956\n",
            "Epoch 13/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.1591 - accuracy: 0.5951\n",
            "Epoch 14/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.1518 - accuracy: 0.5979\n",
            "Epoch 15/25\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.1383 - accuracy: 0.6034\n",
            "Epoch 16/25\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 1.1337 - accuracy: 0.6053\n",
            "Epoch 17/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.1307 - accuracy: 0.6056\n",
            "Epoch 18/25\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.1202 - accuracy: 0.6100\n",
            "Epoch 19/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 1.1182 - accuracy: 0.6132\n",
            "Epoch 20/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 1.1125 - accuracy: 0.6122\n",
            "Epoch 21/25\n",
            "196/196 [==============================] - 39s 201ms/step - loss: 1.1094 - accuracy: 0.6124\n",
            "Epoch 22/25\n",
            "196/196 [==============================] - 39s 201ms/step - loss: 1.1041 - accuracy: 0.6147\n",
            "Epoch 23/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 1.0971 - accuracy: 0.6198\n",
            "Epoch 24/25\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 1.0924 - accuracy: 0.6175\n",
            "Epoch 25/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 1.0884 - accuracy: 0.6218\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.1803 - accuracy: 0.5933\n",
            "Epoch 1/25\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 334734.0000 - accuracy: 0.1443\n",
            "Epoch 2/25\n",
            "196/196 [==============================] - 38s 195ms/step - loss: 195.3639 - accuracy: 0.2287\n",
            "Epoch 3/25\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 87.2340 - accuracy: 0.2443\n",
            "Epoch 4/25\n",
            "196/196 [==============================] - 39s 202ms/step - loss: 55.3873 - accuracy: 0.2582\n",
            "Epoch 5/25\n",
            "196/196 [==============================] - 38s 195ms/step - loss: 38.6350 - accuracy: 0.2712\n",
            "Epoch 6/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 28.7307 - accuracy: 0.2802\n",
            "Epoch 7/25\n",
            "196/196 [==============================] - 38s 195ms/step - loss: 23.0631 - accuracy: 0.2904\n",
            "Epoch 8/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 18.3990 - accuracy: 0.2929\n",
            "Epoch 9/25\n",
            "196/196 [==============================] - 38s 195ms/step - loss: 15.5029 - accuracy: 0.3003\n",
            "Epoch 10/25\n",
            "196/196 [==============================] - 39s 201ms/step - loss: 12.8562 - accuracy: 0.3102\n",
            "Epoch 11/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 11.1159 - accuracy: 0.3161\n",
            "Epoch 12/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 9.2191 - accuracy: 0.3214\n",
            "Epoch 13/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 7.8923 - accuracy: 0.3284\n",
            "Epoch 14/25\n",
            "196/196 [==============================] - 39s 201ms/step - loss: 7.3638 - accuracy: 0.3265\n",
            "Epoch 15/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 6.1766 - accuracy: 0.3354\n",
            "Epoch 16/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 5.6546 - accuracy: 0.3392\n",
            "Epoch 17/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 2501796096.0000 - accuracy: 0.1749\n",
            "Epoch 18/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 4636502.5000 - accuracy: 0.1850\n",
            "Epoch 19/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 1598582.2500 - accuracy: 0.2017\n",
            "Epoch 20/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 1018760.0000 - accuracy: 0.2216\n",
            "Epoch 21/25\n",
            "196/196 [==============================] - 39s 199ms/step - loss: 602331.0000 - accuracy: 0.2431\n",
            "Epoch 22/25\n",
            "196/196 [==============================] - 38s 191ms/step - loss: 490713.1250 - accuracy: 0.2495\n",
            "Epoch 23/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 332120.5000 - accuracy: 0.2588\n",
            "Epoch 24/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 238111.5469 - accuracy: 0.2681\n",
            "Epoch 25/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 188518.4844 - accuracy: 0.2739\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 130512.4609 - accuracy: 0.3001\n",
            "Epoch 1/25\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 6912109.5000 - accuracy: 0.1625\n",
            "Epoch 2/25\n",
            "196/196 [==============================] - 39s 201ms/step - loss: 4835.8521 - accuracy: 0.2416\n",
            "Epoch 3/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 2403.7087 - accuracy: 0.2611\n",
            "Epoch 4/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 1541.9069 - accuracy: 0.2774\n",
            "Epoch 5/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 959.7768 - accuracy: 0.3003\n",
            "Epoch 6/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 705.9601 - accuracy: 0.3004\n",
            "Epoch 7/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 527.2374 - accuracy: 0.3123\n",
            "Epoch 8/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 410.5250 - accuracy: 0.3171\n",
            "Epoch 9/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 293.4470 - accuracy: 0.3282\n",
            "Epoch 10/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 230.1984 - accuracy: 0.3329\n",
            "Epoch 11/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 557264064.0000 - accuracy: 0.2691\n",
            "Epoch 12/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 52662263808.0000 - accuracy: 0.1493\n",
            "Epoch 13/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 38485600.0000 - accuracy: 0.2103\n",
            "Epoch 14/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 19286362.0000 - accuracy: 0.2229\n",
            "Epoch 15/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 13141334.0000 - accuracy: 0.2367\n",
            "Epoch 16/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 11588820.0000 - accuracy: 0.2297\n",
            "Epoch 17/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 8328669.5000 - accuracy: 0.2465\n",
            "Epoch 18/25\n",
            "196/196 [==============================] - 37s 190ms/step - loss: 6113805.0000 - accuracy: 0.2512\n",
            "Epoch 19/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 5406016.5000 - accuracy: 0.2424\n",
            "Epoch 20/25\n",
            "196/196 [==============================] - 37s 190ms/step - loss: 4881041.5000 - accuracy: 0.2470\n",
            "Epoch 21/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 3793225.7500 - accuracy: 0.2576\n",
            "Epoch 22/25\n",
            "196/196 [==============================] - 38s 191ms/step - loss: 4177497.5000 - accuracy: 0.2489\n",
            "Epoch 23/25\n",
            "196/196 [==============================] - 38s 191ms/step - loss: 3674861.7500 - accuracy: 0.2514\n",
            "Epoch 24/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 3125347.0000 - accuracy: 0.2577\n",
            "Epoch 25/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 3456520.7500 - accuracy: 0.2500\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 4709545.0000 - accuracy: 0.2276\n",
            "Epoch 1/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 66667232.0000 - accuracy: 0.1504\n",
            "Epoch 2/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 43000.0430 - accuracy: 0.2277\n",
            "Epoch 3/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 22993.6777 - accuracy: 0.2479\n",
            "Epoch 4/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 14585.4346 - accuracy: 0.2710\n",
            "Epoch 5/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 9929.2031 - accuracy: 0.2840\n",
            "Epoch 6/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 6431.8809 - accuracy: 0.3010\n",
            "Epoch 7/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 4639.3164 - accuracy: 0.3085\n",
            "Epoch 8/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 3821.1875 - accuracy: 0.3084\n",
            "Epoch 9/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 3032.3633 - accuracy: 0.3129\n",
            "Epoch 10/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 2370.2478 - accuracy: 0.3228\n",
            "Epoch 11/25\n",
            "196/196 [==============================] - 39s 201ms/step - loss: 2080.4585 - accuracy: 0.3209\n",
            "Epoch 12/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 1567.2726 - accuracy: 0.3285\n",
            "Epoch 13/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 1281.3734 - accuracy: 0.3336\n",
            "Epoch 14/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 1159.7936 - accuracy: 0.3312\n",
            "Epoch 15/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 1003.0252 - accuracy: 0.3356\n",
            "Epoch 16/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 1000.0932 - accuracy: 0.3316\n",
            "Epoch 17/25\n",
            "196/196 [==============================] - 39s 200ms/step - loss: 740.7616 - accuracy: 0.3380\n",
            "Epoch 18/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 2450.0396 - accuracy: 0.2909\n",
            "Epoch 19/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 747.5981 - accuracy: 0.3569\n",
            "Epoch 20/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 395389403136.0000 - accuracy: 0.2043\n",
            "Epoch 21/25\n",
            "196/196 [==============================] - 39s 199ms/step - loss: 1018135680.0000 - accuracy: 0.1767\n",
            "Epoch 22/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 210895440.0000 - accuracy: 0.1766\n",
            "Epoch 23/25\n",
            "196/196 [==============================] - 39s 199ms/step - loss: 131338024.0000 - accuracy: 0.1874\n",
            "Epoch 24/25\n",
            "196/196 [==============================] - 38s 193ms/step - loss: 93079344.0000 - accuracy: 0.1987\n",
            "Epoch 25/25\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 64403376.0000 - accuracy: 0.2049\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 63097656.0000 - accuracy: 0.1618\n",
            "Epoch 1/25\n",
            "196/196 [==============================] - 42s 209ms/step - loss: 301116544.0000 - accuracy: 0.1540\n",
            "Epoch 2/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 155398.9062 - accuracy: 0.2525\n",
            "Epoch 3/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 73903.2578 - accuracy: 0.2779\n",
            "Epoch 4/25\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 44074.5469 - accuracy: 0.2931\n",
            "Epoch 5/25\n",
            "196/196 [==============================] - 38s 195ms/step - loss: 30762.0664 - accuracy: 0.3013\n",
            "Epoch 6/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 23285.7832 - accuracy: 0.3087\n",
            "Epoch 7/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 17346.5898 - accuracy: 0.3151\n",
            "Epoch 8/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 14089.8174 - accuracy: 0.3258\n",
            "Epoch 9/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 11340.0977 - accuracy: 0.3266\n",
            "Epoch 10/25\n",
            "196/196 [==============================] - 39s 199ms/step - loss: 9020.1416 - accuracy: 0.3379\n",
            "Epoch 11/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 7177.1743 - accuracy: 0.3473\n",
            "Epoch 12/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 6487.2480 - accuracy: 0.3421\n",
            "Epoch 13/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 5493.5898 - accuracy: 0.3454\n",
            "Epoch 14/25\n",
            "196/196 [==============================] - 39s 199ms/step - loss: 4809.7368 - accuracy: 0.3499\n",
            "Epoch 15/25\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 5355.5576 - accuracy: 0.3338\n",
            "Epoch 16/25\n",
            "196/196 [==============================] - 39s 199ms/step - loss: 3418.4893 - accuracy: 0.3560\n",
            "Epoch 17/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 879474507776.0000 - accuracy: 0.1627\n",
            "Epoch 18/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 656336832.0000 - accuracy: 0.1875\n",
            "Epoch 19/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 211232880.0000 - accuracy: 0.2010\n",
            "Epoch 20/25\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 196586800.0000 - accuracy: 0.2052\n",
            "Epoch 21/25\n",
            "196/196 [==============================] - 38s 191ms/step - loss: 134619728.0000 - accuracy: 0.2124\n",
            "Epoch 22/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 114950336.0000 - accuracy: 0.2156\n",
            "Epoch 23/25\n",
            "196/196 [==============================] - 39s 197ms/step - loss: 88450528.0000 - accuracy: 0.2237\n",
            "Epoch 24/25\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 83433816.0000 - accuracy: 0.2234\n",
            "Epoch 25/25\n",
            "196/196 [==============================] - 37s 190ms/step - loss: 62150812.0000 - accuracy: 0.2277\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 40894228.0000 - accuracy: 0.2287\n"
          ]
        }
      ],
      "source": [
        "sc=[]\n",
        "a=[]\n",
        "for i in np.arange(0.001,1,0.2):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(6, kernel_size=(5,5), strides=(1, 1), input_shape=(32,32,3)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "        model.add(Conv2D(16, kernel_size=(5,5), strides=(1,1)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "        model.add(Conv2D(120, kernel_size=(5,5)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(84))\n",
        "        model.add(Dense(10))\n",
        "        model.add(Activation('softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=i), metrics=['accuracy'])\n",
        "        model.fit(X_train,Y_train, batch_size=256, epochs=25)\n",
        "        score = model.evaluate(X_test, Y_test)\n",
        "        sc.append(score)\n",
        "        a.append((i))"
      ],
      "id": "140ceeb9"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1c932e4f"
      },
      "outputs": [],
      "source": [
        "l=[sc[i][1] for i in range(len(sc))]"
      ],
      "id": "1c932e4f"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d16f8181"
      },
      "outputs": [],
      "source": [
        "l1=[str(i) for i in a]"
      ],
      "id": "d16f8181"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "28871d56",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "79718276-eb06-4441-888a-623ee51a8434"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "f = plt.figure()\n",
        "plt.bar(l1,l)\n",
        "f.set_figheight(6)\n",
        "f.set_figwidth(20)"
      ],
      "id": "28871d56"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5acfcdfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4f97fd39-5022-43da-d654-6f8a6ca1ea70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.001'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "l1[l.index(max(l))]"
      ],
      "id": "5acfcdfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1983ec6"
      },
      "source": [
        "### from the plot we can see that the best model for the given data is producing highest accuracy at the learning rate 0.001"
      ],
      "id": "b1983ec6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66a0b979"
      },
      "source": [
        "#### 2.Various batch sizes"
      ],
      "id": "66a0b979"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2aa7aae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03412f8e-5fe8-451c-fc46-c4c3dea7bdc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 1.7666 - accuracy: 0.3890\n",
            "Epoch 2/25\n",
            "500/500 [==============================] - 44s 88ms/step - loss: 1.4958 - accuracy: 0.4722\n",
            "Epoch 3/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.4193 - accuracy: 0.5048\n",
            "Epoch 4/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.4292 - accuracy: 0.5021\n",
            "Epoch 5/25\n",
            "500/500 [==============================] - 44s 87ms/step - loss: 1.3969 - accuracy: 0.5173\n",
            "Epoch 6/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3785 - accuracy: 0.5220\n",
            "Epoch 7/25\n",
            "500/500 [==============================] - 43s 87ms/step - loss: 1.3867 - accuracy: 0.5166\n",
            "Epoch 8/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3621 - accuracy: 0.5312\n",
            "Epoch 9/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 1.3390 - accuracy: 0.5399\n",
            "Epoch 10/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 1.3693 - accuracy: 0.5256\n",
            "Epoch 11/25\n",
            "500/500 [==============================] - 44s 87ms/step - loss: 1.3677 - accuracy: 0.5295\n",
            "Epoch 12/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3605 - accuracy: 0.5285\n",
            "Epoch 13/25\n",
            "500/500 [==============================] - 44s 87ms/step - loss: 1.3304 - accuracy: 0.5424\n",
            "Epoch 14/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3513 - accuracy: 0.5344\n",
            "Epoch 15/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3275 - accuracy: 0.5415\n",
            "Epoch 16/25\n",
            "500/500 [==============================] - 44s 87ms/step - loss: 1.3691 - accuracy: 0.5296\n",
            "Epoch 17/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3365 - accuracy: 0.5402\n",
            "Epoch 18/25\n",
            "500/500 [==============================] - 43s 87ms/step - loss: 1.3288 - accuracy: 0.5417\n",
            "Epoch 19/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3403 - accuracy: 0.5380\n",
            "Epoch 20/25\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 1.3325 - accuracy: 0.5394\n",
            "Epoch 21/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 1.3558 - accuracy: 0.5336\n",
            "Epoch 22/25\n",
            "500/500 [==============================] - 43s 87ms/step - loss: 1.2980 - accuracy: 0.5513\n",
            "Epoch 23/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 1.3248 - accuracy: 0.5449\n",
            "Epoch 24/25\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 276.2791 - accuracy: 0.3435\n",
            "Epoch 25/25\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 2.4521 - accuracy: 0.4635\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.9222 - accuracy: 0.4873\n",
            "Epoch 1/25\n",
            "167/167 [==============================] - 39s 229ms/step - loss: 2.0024 - accuracy: 0.3347\n",
            "Epoch 2/25\n",
            "167/167 [==============================] - 40s 237ms/step - loss: 1.4896 - accuracy: 0.4694\n",
            "Epoch 3/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.4017 - accuracy: 0.5101\n",
            "Epoch 4/25\n",
            "167/167 [==============================] - 40s 238ms/step - loss: 1.3464 - accuracy: 0.5296\n",
            "Epoch 5/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.3230 - accuracy: 0.5388\n",
            "Epoch 6/25\n",
            "167/167 [==============================] - 39s 230ms/step - loss: 1.3127 - accuracy: 0.5408\n",
            "Epoch 7/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.2833 - accuracy: 0.5538\n",
            "Epoch 8/25\n",
            "167/167 [==============================] - 40s 238ms/step - loss: 1.3446 - accuracy: 0.5336\n",
            "Epoch 9/25\n",
            "167/167 [==============================] - 39s 231ms/step - loss: 1.2924 - accuracy: 0.5526\n",
            "Epoch 10/25\n",
            "167/167 [==============================] - 40s 238ms/step - loss: 1.2533 - accuracy: 0.5657\n",
            "Epoch 11/25\n",
            "167/167 [==============================] - 38s 229ms/step - loss: 1.2254 - accuracy: 0.5761\n",
            "Epoch 12/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.2222 - accuracy: 0.5759\n",
            "Epoch 13/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.2991 - accuracy: 0.5540\n",
            "Epoch 14/25\n",
            "167/167 [==============================] - 40s 237ms/step - loss: 1.2311 - accuracy: 0.5721\n",
            "Epoch 15/25\n",
            "167/167 [==============================] - 39s 231ms/step - loss: 1.1851 - accuracy: 0.5925\n",
            "Epoch 16/25\n",
            "167/167 [==============================] - 40s 239ms/step - loss: 1.1978 - accuracy: 0.5890\n",
            "Epoch 17/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.1955 - accuracy: 0.5863\n",
            "Epoch 18/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.1834 - accuracy: 0.5915\n",
            "Epoch 19/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.1873 - accuracy: 0.5925\n",
            "Epoch 20/25\n",
            "167/167 [==============================] - 40s 238ms/step - loss: 1.1819 - accuracy: 0.5918\n",
            "Epoch 21/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.1676 - accuracy: 0.5997\n",
            "Epoch 22/25\n",
            "167/167 [==============================] - 40s 238ms/step - loss: 1.1629 - accuracy: 0.5988\n",
            "Epoch 23/25\n",
            "167/167 [==============================] - 39s 231ms/step - loss: 1.1777 - accuracy: 0.5931\n",
            "Epoch 24/25\n",
            "167/167 [==============================] - 38s 230ms/step - loss: 1.1708 - accuracy: 0.5983\n",
            "Epoch 25/25\n",
            "167/167 [==============================] - 39s 231ms/step - loss: 1.1545 - accuracy: 0.6004\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.2006 - accuracy: 0.5923\n",
            "Epoch 1/25\n",
            "100/100 [==============================] - 38s 372ms/step - loss: 2.5595 - accuracy: 0.2853\n",
            "Epoch 2/25\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 1.5904 - accuracy: 0.4291\n",
            "Epoch 3/25\n",
            "100/100 [==============================] - 37s 374ms/step - loss: 1.4708 - accuracy: 0.4795\n",
            "Epoch 4/25\n",
            "100/100 [==============================] - 38s 375ms/step - loss: 1.4040 - accuracy: 0.5056\n",
            "Epoch 5/25\n",
            "100/100 [==============================] - 38s 375ms/step - loss: 3.7400 - accuracy: 0.4807\n",
            "Epoch 6/25\n",
            "100/100 [==============================] - 39s 388ms/step - loss: 107.0177 - accuracy: 0.1681\n",
            "Epoch 7/25\n",
            "100/100 [==============================] - 37s 375ms/step - loss: 1.7781 - accuracy: 0.3918\n",
            "Epoch 8/25\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 1.5831 - accuracy: 0.4395\n",
            "Epoch 9/25\n",
            "100/100 [==============================] - 37s 374ms/step - loss: 1.5156 - accuracy: 0.4656\n",
            "Epoch 10/25\n",
            "100/100 [==============================] - 37s 372ms/step - loss: 1.4763 - accuracy: 0.4832\n",
            "Epoch 11/25\n",
            "100/100 [==============================] - 37s 372ms/step - loss: 1.4445 - accuracy: 0.4948\n",
            "Epoch 12/25\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 1.4161 - accuracy: 0.5063\n",
            "Epoch 13/25\n",
            "100/100 [==============================] - 37s 371ms/step - loss: 1.3967 - accuracy: 0.5136\n",
            "Epoch 14/25\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 1.3786 - accuracy: 0.5193\n",
            "Epoch 15/25\n",
            "100/100 [==============================] - 37s 374ms/step - loss: 1.3612 - accuracy: 0.5255\n",
            "Epoch 16/25\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 1.3480 - accuracy: 0.5300\n",
            "Epoch 17/25\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 1.3334 - accuracy: 0.5361\n",
            "Epoch 18/25\n",
            "100/100 [==============================] - 39s 387ms/step - loss: 1.3177 - accuracy: 0.5417\n",
            "Epoch 19/25\n",
            "100/100 [==============================] - 38s 375ms/step - loss: 1.3044 - accuracy: 0.5462\n",
            "Epoch 20/25\n",
            "100/100 [==============================] - 39s 387ms/step - loss: 1.2976 - accuracy: 0.5475\n",
            "Epoch 21/25\n",
            "100/100 [==============================] - 38s 377ms/step - loss: 1.2827 - accuracy: 0.5536\n",
            "Epoch 22/25\n",
            "100/100 [==============================] - 38s 376ms/step - loss: 1.2724 - accuracy: 0.5572\n",
            "Epoch 23/25\n",
            "100/100 [==============================] - 37s 374ms/step - loss: 1.2649 - accuracy: 0.5608\n",
            "Epoch 24/25\n",
            "100/100 [==============================] - 39s 388ms/step - loss: 1.2559 - accuracy: 0.5631\n",
            "Epoch 25/25\n",
            "100/100 [==============================] - 38s 375ms/step - loss: 1.2529 - accuracy: 0.5646\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.3079 - accuracy: 0.5459\n",
            "Epoch 1/25\n",
            "72/72 [==============================] - 37s 508ms/step - loss: 2.4236 - accuracy: 0.2492\n",
            "Epoch 2/25\n",
            "72/72 [==============================] - 36s 503ms/step - loss: 1.6326 - accuracy: 0.4220\n",
            "Epoch 3/25\n",
            "72/72 [==============================] - 36s 504ms/step - loss: 1.4974 - accuracy: 0.4707\n",
            "Epoch 4/25\n",
            "72/72 [==============================] - 36s 505ms/step - loss: 1.4046 - accuracy: 0.5066\n",
            "Epoch 5/25\n",
            "72/72 [==============================] - 36s 506ms/step - loss: 1.3523 - accuracy: 0.5256\n",
            "Epoch 6/25\n",
            "72/72 [==============================] - 38s 524ms/step - loss: 1.3140 - accuracy: 0.5429\n",
            "Epoch 7/25\n",
            "72/72 [==============================] - 36s 505ms/step - loss: 1.2836 - accuracy: 0.5532\n",
            "Epoch 8/25\n",
            "72/72 [==============================] - 37s 509ms/step - loss: 1.2500 - accuracy: 0.5655\n",
            "Epoch 9/25\n",
            "72/72 [==============================] - 38s 525ms/step - loss: 1.2407 - accuracy: 0.5689\n",
            "Epoch 10/25\n",
            "72/72 [==============================] - 36s 506ms/step - loss: 1.2228 - accuracy: 0.5761\n",
            "Epoch 11/25\n",
            "72/72 [==============================] - 37s 510ms/step - loss: 1.2349 - accuracy: 0.5724\n",
            "Epoch 12/25\n",
            "72/72 [==============================] - 38s 527ms/step - loss: 1.2128 - accuracy: 0.5800\n",
            "Epoch 13/25\n",
            "72/72 [==============================] - 37s 510ms/step - loss: 1.1825 - accuracy: 0.5908\n",
            "Epoch 14/25\n",
            "72/72 [==============================] - 37s 511ms/step - loss: 1.1846 - accuracy: 0.5901\n",
            "Epoch 15/25\n",
            "72/72 [==============================] - 38s 529ms/step - loss: 1.2160 - accuracy: 0.5781\n",
            "Epoch 16/25\n",
            "72/72 [==============================] - 37s 511ms/step - loss: 1.1965 - accuracy: 0.5847\n",
            "Epoch 17/25\n",
            "72/72 [==============================] - 37s 511ms/step - loss: 1.1696 - accuracy: 0.5913\n",
            "Epoch 18/25\n",
            "72/72 [==============================] - 38s 528ms/step - loss: 1.1724 - accuracy: 0.5920\n",
            "Epoch 19/25\n",
            "72/72 [==============================] - 37s 508ms/step - loss: 1.1663 - accuracy: 0.5938\n",
            "Epoch 20/25\n",
            "72/72 [==============================] - 38s 526ms/step - loss: 1.1745 - accuracy: 0.5946\n",
            "Epoch 21/25\n",
            "72/72 [==============================] - 37s 510ms/step - loss: 1.1549 - accuracy: 0.6002\n",
            "Epoch 22/25\n",
            "72/72 [==============================] - 37s 507ms/step - loss: 1.1500 - accuracy: 0.6029\n",
            "Epoch 23/25\n",
            "72/72 [==============================] - 37s 510ms/step - loss: 1.1550 - accuracy: 0.6004\n",
            "Epoch 24/25\n",
            "72/72 [==============================] - 38s 525ms/step - loss: 1.1519 - accuracy: 0.5997\n",
            "Epoch 25/25\n",
            "72/72 [==============================] - 33s 455ms/step - loss: 1.1362 - accuracy: 0.6087\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.2158 - accuracy: 0.5795\n",
            "Epoch 1/25\n",
            "56/56 [==============================] - 37s 648ms/step - loss: 3.1900 - accuracy: 0.1875\n",
            "Epoch 2/25\n",
            "56/56 [==============================] - 35s 627ms/step - loss: 1.7666 - accuracy: 0.3718\n",
            "Epoch 3/25\n",
            "56/56 [==============================] - 35s 621ms/step - loss: 1.6071 - accuracy: 0.4236\n",
            "Epoch 4/25\n",
            "56/56 [==============================] - 35s 626ms/step - loss: 1.5019 - accuracy: 0.4642\n",
            "Epoch 5/25\n",
            "56/56 [==============================] - 37s 656ms/step - loss: 1.4574 - accuracy: 0.4815\n",
            "Epoch 6/25\n",
            "56/56 [==============================] - 36s 634ms/step - loss: 1.4217 - accuracy: 0.4964\n",
            "Epoch 7/25\n",
            "56/56 [==============================] - 37s 659ms/step - loss: 1.3811 - accuracy: 0.5139\n",
            "Epoch 8/25\n",
            "56/56 [==============================] - 35s 630ms/step - loss: 1.3526 - accuracy: 0.5268\n",
            "Epoch 9/25\n",
            "56/56 [==============================] - 36s 636ms/step - loss: 1.3293 - accuracy: 0.5342\n",
            "Epoch 10/25\n",
            "56/56 [==============================] - 36s 635ms/step - loss: 1.2887 - accuracy: 0.5492\n",
            "Epoch 11/25\n",
            "56/56 [==============================] - 37s 660ms/step - loss: 1.2793 - accuracy: 0.5518\n",
            "Epoch 12/25\n",
            "56/56 [==============================] - 36s 635ms/step - loss: 1.2648 - accuracy: 0.5584\n",
            "Epoch 13/25\n",
            "56/56 [==============================] - 37s 660ms/step - loss: 1.2312 - accuracy: 0.5728\n",
            "Epoch 14/25\n",
            "56/56 [==============================] - 36s 637ms/step - loss: 1.2236 - accuracy: 0.5754\n",
            "Epoch 15/25\n",
            "56/56 [==============================] - 35s 633ms/step - loss: 1.2274 - accuracy: 0.5758\n",
            "Epoch 16/25\n",
            "56/56 [==============================] - 36s 639ms/step - loss: 1.2339 - accuracy: 0.5711\n",
            "Epoch 17/25\n",
            "56/56 [==============================] - 37s 658ms/step - loss: 1.1841 - accuracy: 0.5900\n",
            "Epoch 18/25\n",
            "56/56 [==============================] - 36s 635ms/step - loss: 767.7600 - accuracy: 0.1292\n",
            "Epoch 19/25\n",
            "56/56 [==============================] - 37s 657ms/step - loss: 20.1808 - accuracy: 0.2137\n",
            "Epoch 20/25\n",
            "56/56 [==============================] - 36s 634ms/step - loss: 3.2561 - accuracy: 0.3088\n",
            "Epoch 21/25\n",
            "56/56 [==============================] - 36s 635ms/step - loss: 2.4078 - accuracy: 0.3420\n",
            "Epoch 22/25\n",
            "56/56 [==============================] - 35s 633ms/step - loss: 2.0690 - accuracy: 0.3684\n",
            "Epoch 23/25\n",
            "56/56 [==============================] - 37s 666ms/step - loss: 1.8795 - accuracy: 0.3910\n",
            "Epoch 24/25\n",
            "56/56 [==============================] - 36s 635ms/step - loss: 1.7556 - accuracy: 0.4097\n",
            "Epoch 25/25\n",
            "56/56 [==============================] - 37s 662ms/step - loss: 1.6677 - accuracy: 0.4278\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.6464 - accuracy: 0.4392\n"
          ]
        }
      ],
      "source": [
        "sc=[]\n",
        "a=[]\n",
        "for j in np.arange(100,1000,200):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(6, kernel_size=(5,5), strides=(1, 1), input_shape=(32,32,3)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "        model.add(Conv2D(16, kernel_size=(5,5), strides=(1,1)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "        model.add(Conv2D(120, kernel_size=(5,5)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(84))\n",
        "        model.add(Dense(10))\n",
        "        model.add(Activation('softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
        "        model.fit(X_train,Y_train, batch_size=j, epochs=25)\n",
        "        score = model.evaluate(X_test, Y_test)\n",
        "        sc.append(score)\n",
        "        a.append((j))"
      ],
      "id": "2aa7aae4"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "abf282b7"
      },
      "outputs": [],
      "source": [
        "l=[sc[i][1] for i in range(len(sc))]"
      ],
      "id": "abf282b7"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "690aae9d"
      },
      "outputs": [],
      "source": [
        "l1=[str(i) for i in a]"
      ],
      "id": "690aae9d"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d41c68f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "746757ca-bc37-4b10-8c95-7f5222a575fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "f = plt.figure()\n",
        "plt.bar(l1,l)\n",
        "f.set_figheight(6)\n",
        "f.set_figwidth(20)"
      ],
      "id": "d41c68f3"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "e7ee3715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "19ebc850-1e52-40f0-9809-c45d087b903f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'300'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "l1[l.index(max(l))]"
      ],
      "id": "e7ee3715"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c5cb41a"
      },
      "source": [
        "### from the plot we can see that the best model for the given data is producing highest accuracy at the batch size 300"
      ],
      "id": "8c5cb41a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39af9779"
      },
      "source": [
        "#### 4. Feed forward network"
      ],
      "id": "39af9779"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dbbcb697"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=5, padding=\"same\",input_shape=(32,32,3)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "id": "dbbcb697"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d66ce099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bccc5c5-f712-4966-8b77-59e738a4221e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 374s 478ms/step - loss: 1087.5807 - accuracy: 0.1005\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 372s 476ms/step - loss: 2.3110 - accuracy: 0.0971\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 371s 475ms/step - loss: 2.3119 - accuracy: 0.0975\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 373s 477ms/step - loss: 2.3116 - accuracy: 0.1020\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 375s 480ms/step - loss: 2.3118 - accuracy: 0.1005\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 368s 470ms/step - loss: 2.3125 - accuracy: 0.0981\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 366s 468ms/step - loss: 2.3122 - accuracy: 0.1005\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 367s 470ms/step - loss: 2.3113 - accuracy: 0.0990\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 366s 468ms/step - loss: 2.3118 - accuracy: 0.1029\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 363s 465ms/step - loss: 2.3116 - accuracy: 0.1009\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 363s 465ms/step - loss: 2.3114 - accuracy: 0.0980\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 365s 466ms/step - loss: 2.3118 - accuracy: 0.0995\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 365s 466ms/step - loss: 2.3114 - accuracy: 0.0994\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 368s 471ms/step - loss: 2.3116 - accuracy: 0.1009\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 364s 465ms/step - loss: 2.3105 - accuracy: 0.0992\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 361s 462ms/step - loss: 2.3120 - accuracy: 0.0996\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 361s 461ms/step - loss: 2.3120 - accuracy: 0.0999\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 360s 461ms/step - loss: 2.3103 - accuracy: 0.0979\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 362s 463ms/step - loss: 2.3114 - accuracy: 0.0997\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 362s 463ms/step - loss: 2.3112 - accuracy: 0.0985\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 362s 463ms/step - loss: 2.3115 - accuracy: 0.1008\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 364s 466ms/step - loss: 2.3120 - accuracy: 0.1001\n",
            "Epoch 23/25\n",
            "575/782 [=====================>........] - ETA: 1:37 - loss: 2.3115 - accuracy: 0.1017"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.1),metrics=[\"accuracy\"])\n",
        "model.fit(X_train,Y_train,batch_size=64, epochs=25)\n",
        "score = model.evaluate(X_test, Y_test)\n",
        "print(score[1])"
      ],
      "id": "d66ce099"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eda12ed3"
      },
      "source": [
        "###### a.we can see that the accuracy of feed forward network is quite low when compared to lenet implementation there is a margin of 50% in accuracy difference "
      ],
      "id": "eda12ed3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704cc604"
      },
      "source": [
        "###### b.The paramaters used for this are almost similar but there is quite difference in architechture or layers.   \n",
        "###### So from this we can conclude that, the imapact of parameters is minimal until we have well layered model implementation"
      ],
      "id": "704cc604"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "e7574175"
      },
      "outputs": [],
      "source": [],
      "id": "e7574175"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "334b7df9",
        "1ab45f07"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}